{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLC5qoafXMtX",
    "outputId": "1c522f18-d6e5-42f1-a091-12017896b30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tAb77yZ9fzMG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {  'id'         :  np.int16,   \n",
    "  'item_id'         :  np.int16,   \n",
    "    'dept_id'       :  np.int16,   \n",
    "    'cat_id'        :  np.int16,  \n",
    "    'store_id'      :  np.int16,\n",
    "     'd'            :  np.int16, \n",
    "    'state_id'      :  np.int8,  \n",
    "    'sale'          :  np.int16,  \n",
    "    'year'          :  np.int8, \n",
    "    'event_name_1'  :  np.int8,   \n",
    "   'event_type_1'   :  np.int8,   \n",
    "   'event_name_2'   :  np.int8,   \n",
    "   'event_type_2'   :  np.int8,   \n",
    "   'snap_CA'        :  np.int8,   \n",
    "   'snap_TX'        :  np.int8,   \n",
    "   'snap_WI'        :  np.int8,   \n",
    "   'sell_price'     :  np.float32,\n",
    "   'day'            :  np.int8,   \n",
    "   'quarter'        :  np.int8,  \n",
    "   'lag_49'          :  np.int16,  \n",
    "   'lag_56'          :  np.int16,  \n",
    "   'lag_63'         :  np.int16,  \n",
    "   'lag_70'         :  np.int16,\n",
    "   'lag_77'          :  np.int16, \n",
    "   'lag_28'         :  np.int16,  \n",
    "   'lag_35'         :  np.int16,\n",
    "   'lag_42'         :  np.int16,  \n",
    "   'roll_mean_7'    :  np.float16,\n",
    "   'roll_mean_14'   :  np.float16,\n",
    "    'roll_mean_28'    :  np.float16,\n",
    "   'roll_mean_49'   :  np.float16,\n",
    "    'roll_std_7'    :  np.float16,\n",
    "   'roll_std_14'   :  np.float16,\n",
    "    'roll_std_28'    :  np.float16,\n",
    "   'roll_std_49'   :  np.float16,\n",
    "   'expanding_mean_item':  np.float32,\n",
    "   'mean_id_sold'       :  np.float16,\n",
    "   'std_id_sold'      : np.float16,\n",
    "   'mean_id_price'    : np.float16,\n",
    "    'std_id_price'    : np.float16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_fe2.csv',dtype = d)\n",
    "\n",
    "train = df[(df['d']>1000)&(df['d']<=1913)]\n",
    "train.to_csv('train_df.csv',index=False)\n",
    "\n",
    "test = df[(df['d']>1913)&(df['d']<=1941)]\n",
    "test.to_csv('test_df.csv',index=False)\n",
    "\n",
    "final_sub = df[df['d']>1941]\n",
    "final_sub.to_csv('final_sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRd4xfMUXSW7"
   },
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_df.csv',dtype = d)\n",
    "train = train[train['d']>1200] #due to system issue did not train on complete dataset\n",
    "val = pd.read_csv('test_df.csv',dtype = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True) #removing these two columns as it was not adding much information\n",
    "y_train = train['sale']\n",
    "x_train = train.drop(columns = ['sale'])\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "y_cv = val['sale']\n",
    "x_cv = val.drop(columns = ['sale'])\n",
    "\n",
    "del val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3qRjg-3Xlag"
   },
   "source": [
    "As we need only positive number prediction and for tweedie distribution, so instead of Linear regression I have proceeded with linear_model.TweedieRegressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "IzqFAUrBXlQ8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:42: RuntimeWarning: invalid value encountered in multiply\n",
      "  temp = d1 * family.deviance_derivative(y, y_pred, weights)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:44: RuntimeWarning: invalid value encountered in matmul\n",
      "  devp = np.concatenate(([temp.sum()], temp @ X))\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: divide by zero encountered in power\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: invalid value encountered in multiply\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:90: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:93: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:247: RuntimeWarning: overflow encountered in power\n",
      "  return np.power(y_pred, self.power)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:42: RuntimeWarning: invalid value encountered in multiply\n",
      "  temp = d1 * family.deviance_derivative(y, y_pred, weights)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:44: RuntimeWarning: invalid value encountered in matmul\n",
      "  devp = np.concatenate(([temp.sum()], temp @ X))\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: divide by zero encountered in power\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: invalid value encountered in multiply\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:247: RuntimeWarning: overflow encountered in power\n",
      "  return np.power(y_pred, self.power)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:90: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:93: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      " 33%|███████████████████████████▎                                                      | 1/3 [29:03<58:06, 1743.23s/it]D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:42: RuntimeWarning: invalid value encountered in multiply\n",
      "  temp = d1 * family.deviance_derivative(y, y_pred, weights)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:44: RuntimeWarning: invalid value encountered in matmul\n",
      "  devp = np.concatenate(([temp.sum()], temp @ X))\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: divide by zero encountered in power\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: invalid value encountered in multiply\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:90: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:93: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:247: RuntimeWarning: overflow encountered in power\n",
      "  return np.power(y_pred, self.power)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:42: RuntimeWarning: invalid value encountered in multiply\n",
      "  temp = d1 * family.deviance_derivative(y, y_pred, weights)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:44: RuntimeWarning: invalid value encountered in matmul\n",
      "  devp = np.concatenate(([temp.sum()], temp @ X))\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: divide by zero encountered in power\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: invalid value encountered in multiply\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:247: RuntimeWarning: overflow encountered in power\n",
      "  return np.power(y_pred, self.power)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:90: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:93: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      " 67%|█████████████████████████████████████████████████████▎                          | 2/3 [1:03:11<32:01, 1921.92s/it]D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:42: RuntimeWarning: invalid value encountered in multiply\n",
      "  temp = d1 * family.deviance_derivative(y, y_pred, weights)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:44: RuntimeWarning: invalid value encountered in matmul\n",
      "  devp = np.concatenate(([temp.sum()], temp @ X))\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: divide by zero encountered in power\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: invalid value encountered in multiply\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:90: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:93: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:247: RuntimeWarning: overflow encountered in power\n",
      "  return np.power(y_pred, self.power)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:42: RuntimeWarning: invalid value encountered in multiply\n",
      "  temp = d1 * family.deviance_derivative(y, y_pred, weights)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:44: RuntimeWarning: invalid value encountered in matmul\n",
      "  devp = np.concatenate(([temp.sum()], temp @ X))\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: divide by zero encountered in power\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:330: RuntimeWarning: invalid value encountered in multiply\n",
      "  - y * np.power(y_pred, 1 - p) / (1 - p)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:247: RuntimeWarning: overflow encountered in power\n",
      "  return np.power(y_pred, self.power)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:90: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\link.py:93: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(lin_pred)\n",
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py:132: RuntimeWarning: overflow encountered in multiply\n",
      "  return -2 * (y - y_pred) / self.unit_variance(y_pred)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 3/3 [1:40:13<00:00, 2004.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_cv</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2.364561</td>\n",
       "      <td>2.242338</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3.598671</td>\n",
       "      <td>3.646440</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3.598671</td>\n",
       "      <td>3.646440</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2.364444</td>\n",
       "      <td>2.242096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3.598671</td>\n",
       "      <td>3.646440</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3.598671</td>\n",
       "      <td>3.646440</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2.365042</td>\n",
       "      <td>2.242062</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3.598671</td>\n",
       "      <td>3.646440</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.50</td>\n",
       "      <td>3.598671</td>\n",
       "      <td>3.646440</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  RMSE_train   RMSE_cv  power\n",
       "0   0.01    2.364561  2.242338    0.0\n",
       "1   0.01    3.598671  3.646440    1.1\n",
       "2   0.01    3.598671  3.646440    1.5\n",
       "3   0.10    2.364444  2.242096    0.0\n",
       "4   0.10    3.598671  3.646440    1.1\n",
       "5   0.10    3.598671  3.646440    1.5\n",
       "6   0.50    2.365042  2.242062    0.0\n",
       "7   0.50    3.598671  3.646440    1.1\n",
       "8   0.50    3.598671  3.646440    1.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "params = {'alpha' : [0.01,0.1,0.5],\n",
    "         'power': [0,1.1,1.5]}\n",
    "\n",
    "alpha = []\n",
    "power = []\n",
    "RMSE_train = []\n",
    "RMSE_cv = []\n",
    "\n",
    "score = pd.DataFrame()\n",
    "for a in tqdm(params['alpha']):\n",
    "    for p in params['power']:\n",
    "        reg = linear_model.TweedieRegressor(alpha=a,power=p)\n",
    "        reg.fit(x_train,y_train)\n",
    "        y_pred = reg.predict(x_train)\n",
    "        y_cv_pred = reg.predict(x_cv)\n",
    "        alpha.append(a)\n",
    "        power.append(p)\n",
    "        RMSE_train.append(mean_squared_error(y_train.to_numpy(), y_pred,squared=False))\n",
    "        RMSE_cv.append(mean_squared_error(y_cv.to_numpy(), y_cv_pred,squared=False))\n",
    "score['alpha'] = alpha\n",
    "score['RMSE_train'] = RMSE_train\n",
    "score['RMSE_cv'] = RMSE_cv\n",
    "score['power'] = power\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:323: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data RMSE is 2.3644518331909614\n",
      "CV data RMSE is 2.24183644561049\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#training with best parameters\n",
    "reg = linear_model.TweedieRegressor(alpha=0.05)\n",
    "reg.fit(x_train,y_train)\n",
    "pred = reg.predict(x_train)\n",
    "y_pred = reg.predict(x_cv)\n",
    "print('Train data RMSE is',mean_squared_error(y_train.to_numpy(), pred,squared=False))\n",
    "print('CV data RMSE is',mean_squared_error(y_cv.to_numpy(), y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('final_sub.csv',dtype = d) #d_1942 to d_1969 dataframe\n",
    "f.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_f = f['sale']\n",
    "x_test_f = f.drop(columns = ['sale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred_f = reg.predict(x_test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private score - 1.35166"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Alpha Value is 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_df.csv',dtype = d)\n",
    "train = train[train['d']>1200] #due to system issue did not train on complete dataset\n",
    "val = pd.read_csv('test_df.csv',dtype = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "y_train = train['sale']\n",
    "x_train = train.drop(columns = ['sale'])\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "y_cv = val['sale']\n",
    "x_cv = val.drop(columns = ['sale'])\n",
    "\n",
    "del val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data RMSE is 0.5620238237445486\n",
      "CV data RMSE is 3.44992827292165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr =  DecisionTreeRegressor(criterion='poisson')\n",
    "dtr.fit(x_train,y_train)\n",
    "pred = dtr.predict(x_train)\n",
    "y_pred = dtr.predict(x_cv)\n",
    "print('Train data RMSE is',mean_squared_error(y_train.to_numpy(), pred,squared=False))\n",
    "print('CV data RMSE is',mean_squared_error(y_cv.to_numpy(), y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_df.csv',dtype = d)\n",
    "train = train[train['d']>1200] #due to system issue did not train on complete dataset\n",
    "val = pd.read_csv('test_df.csv',dtype = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "y_train = train['sale']\n",
    "x_train = train.drop(columns = ['sale'])\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "y_cv = val['sale']\n",
    "x_cv = val.drop(columns = ['sale'])\n",
    "\n",
    "del val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data RMSE is 3.5767544603150205\n",
      "CV data RMSE is 3.644474562649022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf =  RandomForestRegressor(criterion='poisson',n_estimators = 100,max_depth = 6,n_jobs = -1)\n",
    "rf.fit(x_train,y_train)\n",
    "pred = rf.predict(x_train)\n",
    "y_pred = rf.predict(x_cv)\n",
    "print('Train data RMSE is',mean_squared_error(y_train.to_numpy(), pred,squared=False))\n",
    "print('CV data RMSE is',mean_squared_error(y_cv.to_numpy(), y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('final_sub.csv',dtype = d) #d_1942 to d_1969 dataframe\n",
    "f.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_f = f['sale']\n",
    "x_test_f = f.drop(columns = ['sale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred_f = rf.predict(x_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(rf, open('RF_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private Score - 2.27301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with hyperparamter tuning both DT and RF model can be imporved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_df.csv',dtype = d)\n",
    "train = train[train['d']>1200] #due to system issue did not train on complete dataset\n",
    "val = pd.read_csv('test_df.csv',dtype = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "y_train = train['sale']\n",
    "x_train = train.drop(columns = ['sale'])\n",
    "    \n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "y_cv = val['sale']\n",
    "x_cv = val.drop(columns = ['sale'])\n",
    "del val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code can be used for hyperparamter tuning\n",
    "'''params = {\n",
    "    'learning_rate': [0.01, 0.1,0.3],\n",
    "    'max_depth': [10,30,50],\n",
    "    'n_estimators' : [100, 200],\n",
    "}\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "learning_rate = []\n",
    "n_est = []\n",
    "depth = []\n",
    "RMSE_train = []\n",
    "RMSE_cv = []\n",
    "score = pd.DataFrame()\n",
    "for lr in tqdm(params['learning_rate']):\n",
    "    for n_e in params['n_estimators']:\n",
    "        for d in params['max_depth']:\n",
    "            xgbr = XGBRegressor(learning_rate = lr,\n",
    "                                max_depth = d,\n",
    "                                n_estimators = n_e,\n",
    "                                objective = 'reg:tweedie',\n",
    "                                subsample = 0.7,\n",
    "                                colsample_bytree = 0.7,\n",
    "                                n_jobs = 1)\n",
    "            xgbr.fit(x_train,y_train)\n",
    "            y_pred = xgbr.predict(x_train).astype(np.int32)\n",
    "            y_cv_pred = xgbr.predict(x_cv).astype(np.int32)\n",
    "            learning_rate.append(lr)\n",
    "            n_est.append(n_e)\n",
    "            depth.append(d)\n",
    "            RMSE_train.append(mean_squared_error(y_train.to_numpy(), y_pred,squared=False))\n",
    "            RMSE_cv.append(mean_squared_error(y_cv.to_numpy(), y_cv_pred,squared=False))\n",
    "\n",
    "score['learning_rate'] =  learning_rate\n",
    "score['n_estimators'] = n_est\n",
    "score['depth'] = depth\n",
    "score['RMSE_train'] = RMSE_train\n",
    "score['RMSE_cv'] = RMSE_cv\n",
    "score'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\miniconda\\envs\\env\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data RMSE is 2.2657576\n",
      "CV data RMSE is 2.1701338\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#trained with random paramter\n",
    "xgbr = XGBRegressor(learning_rate = 0.15,\n",
    "                    n_estimators = 75,\n",
    "                    objective = 'reg:tweedie',\n",
    "                    subsample = 0.5,\n",
    "                    colsample_bytree = 0.5,\n",
    "                    n_jobs = 1)\n",
    "xgbr.fit(x_train,y_train)\n",
    "pred = xgbr.predict(x_train)\n",
    "y_pred = xgbr.predict(x_cv)\n",
    "print('Train data RMSE is',mean_squared_error(y_train.to_numpy(), pred,squared=False))\n",
    "print('CV data RMSE is',mean_squared_error(y_cv.to_numpy(), y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(xgbr, open('xgboost_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('final_sub.csv',dtype = d) #d_1942 to d_1969 dataframe\n",
    "f.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_f = f['sale']\n",
    "x_test_f = f.drop(columns = ['sale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred_f = xgbr.predict(x_test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private Score - 0.67483\n",
    "Accuracy Can be increased by choosing paramters after hyperparamter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked for the Random Paramter. Did not perform Hyperparamter tuning as it takes lot of time. Instead we can use LightGBM model where it provides similar results in way lesser Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_df.csv',dtype = d)\n",
    "val = pd.read_csv('test_df.csv',dtype = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "y_train = train['sale']\n",
    "x_train = train.drop(columns = ['sale'])\n",
    "\n",
    "cat = ['id','item_id','dept_id','cat_id','store_id','state_id','year',\n",
    "       'event_name_1','event_name_2','event_type_1','event_type_2','snap_CA','snap_TX','snap_WI',\n",
    "      'day','quarter']\n",
    "\n",
    "for ele in cat:  #we need to spacify dtype as category for categorical columns for LightGBM model\n",
    "    x_train[ele] = pd.Series(x_train[ele],dtype='category')\n",
    "    \n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "y_cv = val['sale']\n",
    "x_cv = val.drop(columns = ['sale'])\n",
    "\n",
    "for ele in cat:\n",
    "    x_cv[ele] = pd.Series(x_cv[ele],dtype='category')\n",
    "\n",
    "del val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': [0.1,0.2],\n",
    "    'num_leaves' : [50,100,150],\n",
    "    'min_data_in_leaf' : [50,75,100],\n",
    "    'n_estimators' : [100,200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████▌                                      | 1/2 [5:15:20<5:15:20, 18920.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=75, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=75\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [10:17:56<00:00, 18538.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "learning_rate = []\n",
    "num_leaves = []\n",
    "n_estimators = []\n",
    "RMSE_train = []\n",
    "RMSE_cv = []\n",
    "min_data = []\n",
    "score = pd.DataFrame()\n",
    "for lr in tqdm(params['learning_rate']):\n",
    "    for nl in params['num_leaves']:\n",
    "        for mdl in params['min_data_in_leaf']:\n",
    "            for ne in params['n_estimators']:\n",
    "                lgbm = LGBMRegressor(learning_rate = lr,\n",
    "                                     num_leaves = nl,\n",
    "                                     min_data_in_leaf = mdl,\n",
    "                                     n_estimators = ne,\n",
    "                                     n_jobs= 1)\n",
    "                lgbm.fit(x_train,y_train)\n",
    "                y_pred = lgbm.predict(x_train)\n",
    "                y_cv_pred = lgbm.predict(x_cv)\n",
    "                learning_rate.append(lr)\n",
    "                num_leaves.append(nl)\n",
    "                min_data.append(mdl)\n",
    "                n_estimators.append(ne)\n",
    "                RMSE_train.append(mean_squared_error(y_train.to_numpy(), y_pred,squared=False))\n",
    "                RMSE_cv.append(mean_squared_error(y_cv.to_numpy(), y_cv_pred,squared=False))\n",
    "                \n",
    "score['learning_rate'] =  learning_rate\n",
    "score['n_estimators'] = n_estimators\n",
    "score['num_leaves'] = num_leaves\n",
    "score['min_data'] = min_data\n",
    "score['RMSE_train'] = RMSE_train\n",
    "score['RMSE_cv'] = RMSE_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>min_data</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>2.176974</td>\n",
       "      <td>2.152726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>2.109548</td>\n",
       "      <td>2.147529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>75</td>\n",
       "      <td>2.177344</td>\n",
       "      <td>2.155126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>75</td>\n",
       "      <td>2.115027</td>\n",
       "      <td>2.150168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>2.180125</td>\n",
       "      <td>2.150732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>2.114707</td>\n",
       "      <td>2.143590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>2.115507</td>\n",
       "      <td>2.152499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>2.032287</td>\n",
       "      <td>2.142324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>2.118374</td>\n",
       "      <td>2.152329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>2.038444</td>\n",
       "      <td>2.144834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>2.121452</td>\n",
       "      <td>2.153079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>2.043232</td>\n",
       "      <td>2.146090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>2.077109</td>\n",
       "      <td>2.151163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>1.986617</td>\n",
       "      <td>2.145297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>75</td>\n",
       "      <td>2.081637</td>\n",
       "      <td>2.150058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>75</td>\n",
       "      <td>1.994478</td>\n",
       "      <td>2.142396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>2.085354</td>\n",
       "      <td>2.149355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1.999103</td>\n",
       "      <td>2.142007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>2.114668</td>\n",
       "      <td>2.150748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>2.045886</td>\n",
       "      <td>2.152752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>75</td>\n",
       "      <td>2.116908</td>\n",
       "      <td>2.155531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>75</td>\n",
       "      <td>2.053554</td>\n",
       "      <td>2.156513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>2.119599</td>\n",
       "      <td>2.151618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>2.053452</td>\n",
       "      <td>2.150696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>2.042435</td>\n",
       "      <td>2.153328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>1.963204</td>\n",
       "      <td>2.149612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>2.049621</td>\n",
       "      <td>2.153520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>1.973590</td>\n",
       "      <td>2.153376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>2.051249</td>\n",
       "      <td>2.162352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.972341</td>\n",
       "      <td>2.151675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>1.995545</td>\n",
       "      <td>2.149489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>1.908818</td>\n",
       "      <td>2.154017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>75</td>\n",
       "      <td>2.002829</td>\n",
       "      <td>2.158361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>75</td>\n",
       "      <td>1.911749</td>\n",
       "      <td>2.160546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>2.009191</td>\n",
       "      <td>2.153972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1.923953</td>\n",
       "      <td>2.150267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  n_estimators  num_leaves  min_data  RMSE_train   RMSE_cv\n",
       "0             0.1           100          50        50    2.176974  2.152726\n",
       "1             0.1           200          50        50    2.109548  2.147529\n",
       "2             0.1           100          50        75    2.177344  2.155126\n",
       "3             0.1           200          50        75    2.115027  2.150168\n",
       "4             0.1           100          50       100    2.180125  2.150732\n",
       "5             0.1           200          50       100    2.114707  2.143590\n",
       "6             0.1           100         100        50    2.115507  2.152499\n",
       "7             0.1           200         100        50    2.032287  2.142324\n",
       "8             0.1           100         100        75    2.118374  2.152329\n",
       "9             0.1           200         100        75    2.038444  2.144834\n",
       "10            0.1           100         100       100    2.121452  2.153079\n",
       "11            0.1           200         100       100    2.043232  2.146090\n",
       "12            0.1           100         150        50    2.077109  2.151163\n",
       "13            0.1           200         150        50    1.986617  2.145297\n",
       "14            0.1           100         150        75    2.081637  2.150058\n",
       "15            0.1           200         150        75    1.994478  2.142396\n",
       "16            0.1           100         150       100    2.085354  2.149355\n",
       "17            0.1           200         150       100    1.999103  2.142007\n",
       "18            0.2           100          50        50    2.114668  2.150748\n",
       "19            0.2           200          50        50    2.045886  2.152752\n",
       "20            0.2           100          50        75    2.116908  2.155531\n",
       "21            0.2           200          50        75    2.053554  2.156513\n",
       "22            0.2           100          50       100    2.119599  2.151618\n",
       "23            0.2           200          50       100    2.053452  2.150696\n",
       "24            0.2           100         100        50    2.042435  2.153328\n",
       "25            0.2           200         100        50    1.963204  2.149612\n",
       "26            0.2           100         100        75    2.049621  2.153520\n",
       "27            0.2           200         100        75    1.973590  2.153376\n",
       "28            0.2           100         100       100    2.051249  2.162352\n",
       "29            0.2           200         100       100    1.972341  2.151675\n",
       "30            0.2           100         150        50    1.995545  2.149489\n",
       "31            0.2           200         150        50    1.908818  2.154017\n",
       "32            0.2           100         150        75    2.002829  2.158361\n",
       "33            0.2           200         150        75    1.911749  2.160546\n",
       "34            0.2           100         150       100    2.009191  2.153972\n",
       "35            0.2           200         150       100    1.923953  2.150267"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Parameters with Tweedie as objective Function and Power = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lgbm = LGBMRegressor(learning_rate = 0.1,\n",
    "                     num_leaves = 50,\n",
    "                     n_estimators = 200,\n",
    "                     min_data_in_leaf = 100,\n",
    "                     n_jobs= 1,\n",
    "                    objective = 'tweedie',\n",
    "                    tweedie_variance_power = 1.1)\n",
    "lgbm.fit(x_train,y_train)\n",
    "y_pred = lgbm.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.194704964850303"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " mean_squared_error(y_train.to_numpy(), y_pred,squared=False) #RMSE for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score for test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.166043497142106"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = lgbm.predict(x_cv)\n",
    "print('RMSE score for test data')\n",
    "mean_squared_error(y_cv.to_numpy(), y_pred,squared = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private Score - 0.68122"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without any objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lgbm = LGBMRegressor(learning_rate = 0.1,\n",
    "                     num_leaves = 50,\n",
    "                     n_estimators = 200,\n",
    "                     min_data_in_leaf = 100,\n",
    "                     n_jobs= 1)\n",
    "lgbm.fit(x_train,y_train)\n",
    "y_pred = lgbm.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1147072518697283"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " mean_squared_error(y_train.to_numpy(), y_pred,squared=False) #RMSE for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score for test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.143589670919594"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = lgbm.predict(x_cv)\n",
    "print('RMSE score for test data')\n",
    "mean_squared_error(y_cv.to_numpy(), y_pred,squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lgbm, open('LightGBM_model.sav', 'wb')) #model used in final pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private Score - 0.64121 -> with negative Predictions <br>\n",
    "Private Score - 0.64101 -> After Clipping Negative predictions to Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here trained LightGBM with both Tweedie as objective function and another one with default objective function i.e. Regression. Even though target column is tweedie distributed, LightGBM with tweedie did not perform better as Power value varies from item to item so using any particular power value may lead to accurate prediction only for that particular items whose power value matches. So, instead of tweedie, used default objective function where it performed better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on Last 28+28 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('final_sub.csv',dtype = d) #d_1942 to d_1969 dataframe\n",
    "f.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"col = ['sell_price','lag_28','lag_35','lag_42','lag_49', 'lag_56','lag_63','lag_70', 'lag_77',\\n       'mean_id_price','expanding_mean_item','mean_id_sold',\\n       'roll_mean_28','roll_mean_7','roll_mean_14','roll_std_7','roll_std_14','roll_std_28','std_id_price','std_id_sold']\\nx_test_f[col] = scaler.transform(x_test_f[col])\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_f = f['sale']\n",
    "x_test_f = f.drop(columns = ['sale'])\n",
    "\n",
    "for ele in cat:\n",
    "    x_test_f[ele] = pd.Series(x_test_f[ele],dtype='category')\n",
    "\n",
    "'''col = ['sell_price','lag_28','lag_35','lag_42','lag_49', 'lag_56','lag_63','lag_70', 'lag_77',\n",
    "       'mean_id_price','expanding_mean_item','mean_id_sold',\n",
    "       'roll_mean_28','roll_mean_7','roll_mean_14','roll_std_7','roll_std_14','roll_std_28','std_id_price','std_id_sold']\n",
    "x_test_f[col] = scaler.transform(x_test_f[col])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred_f = lgbm.predict(x_test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance of final LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://www.kaggle.com/ashishpatel26/feature-importance-of-lightgbm#Feature-importance\n",
    "#feature Importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\n",
    "feature_imp = pd.DataFrame(sorted(zip(lgbm.feature_importances_,x_train.columns)), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different model for different department\n",
    "### (Just Tried creating different model for different depatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FOODS_1': 0,\n",
       " 'FOODS_2': 1,\n",
       " 'FOODS_3': 2,\n",
       " 'HOBBIES_1': 3,\n",
       " 'HOBBIES_2': 4,\n",
       " 'HOUSEHOLD_1': 5,\n",
       " 'HOUSEHOLD_2': 6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = pd.read_pickle('label_encoding.pickle')\n",
    "le['dept_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweedie_power = [1.36,1.30,1.48,1.36,1,1.33,1] #https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/153518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [15:32<00:00, 133.24s/it]\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "for key, value in  tqdm(le['dept_id'].items()):\n",
    "    df = pd.read_csv('train_df.csv',dtype = d)\n",
    "    df = df[df['dept_id']==value]\n",
    "    df.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)\n",
    "    y_train = df['sale']\n",
    "    x_train = df.drop(columns = ['sale'])\n",
    "    \n",
    "    cat = ['id','item_id','dept_id','cat_id','store_id','state_id','year',\n",
    "       'event_name_1','event_name_2','event_type_1','event_type_2','snap_CA','snap_TX','snap_WI',\n",
    "      'day','quarter']\n",
    "    for ele in cat:\n",
    "        x_train[ele] = pd.Series(x_train[ele],dtype='category')\n",
    "    del df\n",
    "    \n",
    "    lgbm = LGBMRegressor(learning_rate = 0.1,\n",
    "                         num_leaves = 100,\n",
    "                         min_data_in_leaf = 150,\n",
    "                         n_jobs= 1)\n",
    "    lgbm.fit(x_train,y_train) #Reading each csv file and fitting the model\n",
    "    pickle.dump(lgbm, open('trained model/'+key+'.sav', 'wb')) #saving each model and will be evaluated on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28+28 Days Prediction for Department Wise system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['id','item_id','dept_id','cat_id','store_id','state_id','year',\n",
    "       'event_name_1','event_name_2','event_type_1','event_type_2','snap_CA','snap_TX','snap_WI',\n",
    "      'day','quarter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('test_df.csv',dtype = d)\n",
    "val.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('final_sub.csv',dtype = d) #d_1942 to d_1969 dataframe\n",
    "f.drop(columns = ['roll_mean_49','roll_std_49'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:41<00:00,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data RMSE value is  2.1537995219695936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "#calculating RMSE value on test data with respective dataframe and model\n",
    "y_pred = []\n",
    "y = []\n",
    "for key, value in tqdm(le['dept_id'].items()):\n",
    "    df1 = val[val['dept_id']==value]\n",
    "    df2 = f[f['dept_id']==value]\n",
    "    \n",
    "    y_test = df1['sale']\n",
    "    x_test = df1.drop(columns = ['sale'])\n",
    "\n",
    "    for ele in cat:\n",
    "        x_test[ele] = pd.Series(x_test[ele],dtype='category')\n",
    "\n",
    "    y_test_f = df2['sale']\n",
    "    x_test_f = df2.drop(columns = ['sale'])\n",
    "    \n",
    "    for ele in cat:\n",
    "        x_test_f[ele] = pd.Series(x_test_f[ele],dtype='category')\n",
    "    \n",
    "    model = pickle.load(open('trained model/'+key+'.sav', 'rb'))\n",
    "    y_pred = y_pred + list(model.predict(x_test))\n",
    "    f.loc[f['dept_id']==value,'sale'] = model.predict(x_test_f)\n",
    "    val.loc[val['dept_id']==value,'sale'] = model.predict(x_test)\n",
    "    y = y + y_test.to_list()\n",
    "print('Test Data RMSE value is ',mean_squared_error(y, y_pred, squared = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = val['sale']\n",
    "y_pred_f = f['sale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private Score - 0.69979 <br>\n",
    "For Better score, Hyperparamter should be performed for each Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Submission CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  F1  F2  F3  F4  F5  F6  F7  F8  F9  ...  \\\n",
       "0  HOBBIES_1_001_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
       "1  HOBBIES_1_002_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
       "2  HOBBIES_1_003_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
       "3  HOBBIES_1_004_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
       "4  HOBBIES_1_005_CA_1_validation   0   0   0   0   0   0   0   0   0  ...   \n",
       "\n",
       "   F19  F20  F21  F22  F23  F24  F25  F26  F27  F28  \n",
       "0    0    0    0    0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sample_submission.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'FOODS_1_001_CA_1_evaluation',\n",
       " 1: 'FOODS_1_001_CA_2_evaluation',\n",
       " 2: 'FOODS_1_001_CA_3_evaluation',\n",
       " 3: 'FOODS_1_001_CA_4_evaluation',\n",
       " 4: 'FOODS_1_001_TX_1_evaluation',\n",
       " 5: 'FOODS_1_001_TX_2_evaluation',\n",
       " 6: 'FOODS_1_001_TX_3_evaluation',\n",
       " 7: 'FOODS_1_001_WI_1_evaluation',\n",
       " 8: 'FOODS_1_001_WI_2_evaluation',\n",
       " 9: 'FOODS_1_001_WI_3_evaluation',\n",
       " 10: 'FOODS_1_002_CA_1_evaluation',\n",
       " 11: 'FOODS_1_002_CA_2_evaluation',\n",
       " 12: 'FOODS_1_002_CA_3_evaluation',\n",
       " 13: 'FOODS_1_002_CA_4_evaluation',\n",
       " 14: 'FOODS_1_002_TX_1_evaluation',\n",
       " 15: 'FOODS_1_002_TX_2_evaluation',\n",
       " 16: 'FOODS_1_002_TX_3_evaluation',\n",
       " 17: 'FOODS_1_002_WI_1_evaluation',\n",
       " 18: 'FOODS_1_002_WI_2_evaluation',\n",
       " 19: 'FOODS_1_002_WI_3_evaluation',\n",
       " 20: 'FOODS_1_003_CA_1_evaluation',\n",
       " 21: 'FOODS_1_003_CA_2_evaluation',\n",
       " 22: 'FOODS_1_003_CA_3_evaluation',\n",
       " 23: 'FOODS_1_003_CA_4_evaluation',\n",
       " 24: 'FOODS_1_003_TX_1_evaluation',\n",
       " 25: 'FOODS_1_003_TX_2_evaluation',\n",
       " 26: 'FOODS_1_003_TX_3_evaluation',\n",
       " 27: 'FOODS_1_003_WI_1_evaluation',\n",
       " 28: 'FOODS_1_003_WI_2_evaluation',\n",
       " 29: 'FOODS_1_003_WI_3_evaluation',\n",
       " 30: 'FOODS_1_004_CA_1_evaluation',\n",
       " 31: 'FOODS_1_004_CA_2_evaluation',\n",
       " 32: 'FOODS_1_004_CA_3_evaluation',\n",
       " 33: 'FOODS_1_004_CA_4_evaluation',\n",
       " 34: 'FOODS_1_004_TX_1_evaluation',\n",
       " 35: 'FOODS_1_004_TX_2_evaluation',\n",
       " 36: 'FOODS_1_004_TX_3_evaluation',\n",
       " 37: 'FOODS_1_004_WI_1_evaluation',\n",
       " 38: 'FOODS_1_004_WI_2_evaluation',\n",
       " 39: 'FOODS_1_004_WI_3_evaluation',\n",
       " 40: 'FOODS_1_005_CA_1_evaluation',\n",
       " 41: 'FOODS_1_005_CA_2_evaluation',\n",
       " 42: 'FOODS_1_005_CA_3_evaluation',\n",
       " 43: 'FOODS_1_005_CA_4_evaluation',\n",
       " 44: 'FOODS_1_005_TX_1_evaluation',\n",
       " 45: 'FOODS_1_005_TX_2_evaluation',\n",
       " 46: 'FOODS_1_005_TX_3_evaluation',\n",
       " 47: 'FOODS_1_005_WI_1_evaluation',\n",
       " 48: 'FOODS_1_005_WI_2_evaluation',\n",
       " 49: 'FOODS_1_005_WI_3_evaluation',\n",
       " 50: 'FOODS_1_006_CA_1_evaluation',\n",
       " 51: 'FOODS_1_006_CA_2_evaluation',\n",
       " 52: 'FOODS_1_006_CA_3_evaluation',\n",
       " 53: 'FOODS_1_006_CA_4_evaluation',\n",
       " 54: 'FOODS_1_006_TX_1_evaluation',\n",
       " 55: 'FOODS_1_006_TX_2_evaluation',\n",
       " 56: 'FOODS_1_006_TX_3_evaluation',\n",
       " 57: 'FOODS_1_006_WI_1_evaluation',\n",
       " 58: 'FOODS_1_006_WI_2_evaluation',\n",
       " 59: 'FOODS_1_006_WI_3_evaluation',\n",
       " 60: 'FOODS_1_008_CA_1_evaluation',\n",
       " 61: 'FOODS_1_008_CA_2_evaluation',\n",
       " 62: 'FOODS_1_008_CA_3_evaluation',\n",
       " 63: 'FOODS_1_008_CA_4_evaluation',\n",
       " 64: 'FOODS_1_008_TX_1_evaluation',\n",
       " 65: 'FOODS_1_008_TX_2_evaluation',\n",
       " 66: 'FOODS_1_008_TX_3_evaluation',\n",
       " 67: 'FOODS_1_008_WI_1_evaluation',\n",
       " 68: 'FOODS_1_008_WI_2_evaluation',\n",
       " 69: 'FOODS_1_008_WI_3_evaluation',\n",
       " 70: 'FOODS_1_009_CA_1_evaluation',\n",
       " 71: 'FOODS_1_009_CA_2_evaluation',\n",
       " 72: 'FOODS_1_009_CA_3_evaluation',\n",
       " 73: 'FOODS_1_009_CA_4_evaluation',\n",
       " 74: 'FOODS_1_009_TX_1_evaluation',\n",
       " 75: 'FOODS_1_009_TX_2_evaluation',\n",
       " 76: 'FOODS_1_009_TX_3_evaluation',\n",
       " 77: 'FOODS_1_009_WI_1_evaluation',\n",
       " 78: 'FOODS_1_009_WI_2_evaluation',\n",
       " 79: 'FOODS_1_009_WI_3_evaluation',\n",
       " 80: 'FOODS_1_010_CA_1_evaluation',\n",
       " 81: 'FOODS_1_010_CA_2_evaluation',\n",
       " 82: 'FOODS_1_010_CA_3_evaluation',\n",
       " 83: 'FOODS_1_010_CA_4_evaluation',\n",
       " 84: 'FOODS_1_010_TX_1_evaluation',\n",
       " 85: 'FOODS_1_010_TX_2_evaluation',\n",
       " 86: 'FOODS_1_010_TX_3_evaluation',\n",
       " 87: 'FOODS_1_010_WI_1_evaluation',\n",
       " 88: 'FOODS_1_010_WI_2_evaluation',\n",
       " 89: 'FOODS_1_010_WI_3_evaluation',\n",
       " 90: 'FOODS_1_011_CA_1_evaluation',\n",
       " 91: 'FOODS_1_011_CA_2_evaluation',\n",
       " 92: 'FOODS_1_011_CA_3_evaluation',\n",
       " 93: 'FOODS_1_011_CA_4_evaluation',\n",
       " 94: 'FOODS_1_011_TX_1_evaluation',\n",
       " 95: 'FOODS_1_011_TX_2_evaluation',\n",
       " 96: 'FOODS_1_011_TX_3_evaluation',\n",
       " 97: 'FOODS_1_011_WI_1_evaluation',\n",
       " 98: 'FOODS_1_011_WI_2_evaluation',\n",
       " 99: 'FOODS_1_011_WI_3_evaluation',\n",
       " 100: 'FOODS_1_012_CA_1_evaluation',\n",
       " 101: 'FOODS_1_012_CA_2_evaluation',\n",
       " 102: 'FOODS_1_012_CA_3_evaluation',\n",
       " 103: 'FOODS_1_012_CA_4_evaluation',\n",
       " 104: 'FOODS_1_012_TX_1_evaluation',\n",
       " 105: 'FOODS_1_012_TX_2_evaluation',\n",
       " 106: 'FOODS_1_012_TX_3_evaluation',\n",
       " 107: 'FOODS_1_012_WI_1_evaluation',\n",
       " 108: 'FOODS_1_012_WI_2_evaluation',\n",
       " 109: 'FOODS_1_012_WI_3_evaluation',\n",
       " 110: 'FOODS_1_013_CA_1_evaluation',\n",
       " 111: 'FOODS_1_013_CA_2_evaluation',\n",
       " 112: 'FOODS_1_013_CA_3_evaluation',\n",
       " 113: 'FOODS_1_013_CA_4_evaluation',\n",
       " 114: 'FOODS_1_013_TX_1_evaluation',\n",
       " 115: 'FOODS_1_013_TX_2_evaluation',\n",
       " 116: 'FOODS_1_013_TX_3_evaluation',\n",
       " 117: 'FOODS_1_013_WI_1_evaluation',\n",
       " 118: 'FOODS_1_013_WI_2_evaluation',\n",
       " 119: 'FOODS_1_013_WI_3_evaluation',\n",
       " 120: 'FOODS_1_014_CA_1_evaluation',\n",
       " 121: 'FOODS_1_014_CA_2_evaluation',\n",
       " 122: 'FOODS_1_014_CA_3_evaluation',\n",
       " 123: 'FOODS_1_014_CA_4_evaluation',\n",
       " 124: 'FOODS_1_014_TX_1_evaluation',\n",
       " 125: 'FOODS_1_014_TX_2_evaluation',\n",
       " 126: 'FOODS_1_014_TX_3_evaluation',\n",
       " 127: 'FOODS_1_014_WI_1_evaluation',\n",
       " 128: 'FOODS_1_014_WI_2_evaluation',\n",
       " 129: 'FOODS_1_014_WI_3_evaluation',\n",
       " 130: 'FOODS_1_015_CA_1_evaluation',\n",
       " 131: 'FOODS_1_015_CA_2_evaluation',\n",
       " 132: 'FOODS_1_015_CA_3_evaluation',\n",
       " 133: 'FOODS_1_015_CA_4_evaluation',\n",
       " 134: 'FOODS_1_015_TX_1_evaluation',\n",
       " 135: 'FOODS_1_015_TX_2_evaluation',\n",
       " 136: 'FOODS_1_015_TX_3_evaluation',\n",
       " 137: 'FOODS_1_015_WI_1_evaluation',\n",
       " 138: 'FOODS_1_015_WI_2_evaluation',\n",
       " 139: 'FOODS_1_015_WI_3_evaluation',\n",
       " 140: 'FOODS_1_016_CA_1_evaluation',\n",
       " 141: 'FOODS_1_016_CA_2_evaluation',\n",
       " 142: 'FOODS_1_016_CA_3_evaluation',\n",
       " 143: 'FOODS_1_016_CA_4_evaluation',\n",
       " 144: 'FOODS_1_016_TX_1_evaluation',\n",
       " 145: 'FOODS_1_016_TX_2_evaluation',\n",
       " 146: 'FOODS_1_016_TX_3_evaluation',\n",
       " 147: 'FOODS_1_016_WI_1_evaluation',\n",
       " 148: 'FOODS_1_016_WI_2_evaluation',\n",
       " 149: 'FOODS_1_016_WI_3_evaluation',\n",
       " 150: 'FOODS_1_017_CA_1_evaluation',\n",
       " 151: 'FOODS_1_017_CA_2_evaluation',\n",
       " 152: 'FOODS_1_017_CA_3_evaluation',\n",
       " 153: 'FOODS_1_017_CA_4_evaluation',\n",
       " 154: 'FOODS_1_017_TX_1_evaluation',\n",
       " 155: 'FOODS_1_017_TX_2_evaluation',\n",
       " 156: 'FOODS_1_017_TX_3_evaluation',\n",
       " 157: 'FOODS_1_017_WI_1_evaluation',\n",
       " 158: 'FOODS_1_017_WI_2_evaluation',\n",
       " 159: 'FOODS_1_017_WI_3_evaluation',\n",
       " 160: 'FOODS_1_018_CA_1_evaluation',\n",
       " 161: 'FOODS_1_018_CA_2_evaluation',\n",
       " 162: 'FOODS_1_018_CA_3_evaluation',\n",
       " 163: 'FOODS_1_018_CA_4_evaluation',\n",
       " 164: 'FOODS_1_018_TX_1_evaluation',\n",
       " 165: 'FOODS_1_018_TX_2_evaluation',\n",
       " 166: 'FOODS_1_018_TX_3_evaluation',\n",
       " 167: 'FOODS_1_018_WI_1_evaluation',\n",
       " 168: 'FOODS_1_018_WI_2_evaluation',\n",
       " 169: 'FOODS_1_018_WI_3_evaluation',\n",
       " 170: 'FOODS_1_019_CA_1_evaluation',\n",
       " 171: 'FOODS_1_019_CA_2_evaluation',\n",
       " 172: 'FOODS_1_019_CA_3_evaluation',\n",
       " 173: 'FOODS_1_019_CA_4_evaluation',\n",
       " 174: 'FOODS_1_019_TX_1_evaluation',\n",
       " 175: 'FOODS_1_019_TX_2_evaluation',\n",
       " 176: 'FOODS_1_019_TX_3_evaluation',\n",
       " 177: 'FOODS_1_019_WI_1_evaluation',\n",
       " 178: 'FOODS_1_019_WI_2_evaluation',\n",
       " 179: 'FOODS_1_019_WI_3_evaluation',\n",
       " 180: 'FOODS_1_020_CA_1_evaluation',\n",
       " 181: 'FOODS_1_020_CA_2_evaluation',\n",
       " 182: 'FOODS_1_020_CA_3_evaluation',\n",
       " 183: 'FOODS_1_020_CA_4_evaluation',\n",
       " 184: 'FOODS_1_020_TX_1_evaluation',\n",
       " 185: 'FOODS_1_020_TX_2_evaluation',\n",
       " 186: 'FOODS_1_020_TX_3_evaluation',\n",
       " 187: 'FOODS_1_020_WI_1_evaluation',\n",
       " 188: 'FOODS_1_020_WI_2_evaluation',\n",
       " 189: 'FOODS_1_020_WI_3_evaluation',\n",
       " 190: 'FOODS_1_021_CA_1_evaluation',\n",
       " 191: 'FOODS_1_021_CA_2_evaluation',\n",
       " 192: 'FOODS_1_021_CA_3_evaluation',\n",
       " 193: 'FOODS_1_021_CA_4_evaluation',\n",
       " 194: 'FOODS_1_021_TX_1_evaluation',\n",
       " 195: 'FOODS_1_021_TX_2_evaluation',\n",
       " 196: 'FOODS_1_021_TX_3_evaluation',\n",
       " 197: 'FOODS_1_021_WI_1_evaluation',\n",
       " 198: 'FOODS_1_021_WI_2_evaluation',\n",
       " 199: 'FOODS_1_021_WI_3_evaluation',\n",
       " 200: 'FOODS_1_022_CA_1_evaluation',\n",
       " 201: 'FOODS_1_022_CA_2_evaluation',\n",
       " 202: 'FOODS_1_022_CA_3_evaluation',\n",
       " 203: 'FOODS_1_022_CA_4_evaluation',\n",
       " 204: 'FOODS_1_022_TX_1_evaluation',\n",
       " 205: 'FOODS_1_022_TX_2_evaluation',\n",
       " 206: 'FOODS_1_022_TX_3_evaluation',\n",
       " 207: 'FOODS_1_022_WI_1_evaluation',\n",
       " 208: 'FOODS_1_022_WI_2_evaluation',\n",
       " 209: 'FOODS_1_022_WI_3_evaluation',\n",
       " 210: 'FOODS_1_023_CA_1_evaluation',\n",
       " 211: 'FOODS_1_023_CA_2_evaluation',\n",
       " 212: 'FOODS_1_023_CA_3_evaluation',\n",
       " 213: 'FOODS_1_023_CA_4_evaluation',\n",
       " 214: 'FOODS_1_023_TX_1_evaluation',\n",
       " 215: 'FOODS_1_023_TX_2_evaluation',\n",
       " 216: 'FOODS_1_023_TX_3_evaluation',\n",
       " 217: 'FOODS_1_023_WI_1_evaluation',\n",
       " 218: 'FOODS_1_023_WI_2_evaluation',\n",
       " 219: 'FOODS_1_023_WI_3_evaluation',\n",
       " 220: 'FOODS_1_024_CA_1_evaluation',\n",
       " 221: 'FOODS_1_024_CA_2_evaluation',\n",
       " 222: 'FOODS_1_024_CA_3_evaluation',\n",
       " 223: 'FOODS_1_024_CA_4_evaluation',\n",
       " 224: 'FOODS_1_024_TX_1_evaluation',\n",
       " 225: 'FOODS_1_024_TX_2_evaluation',\n",
       " 226: 'FOODS_1_024_TX_3_evaluation',\n",
       " 227: 'FOODS_1_024_WI_1_evaluation',\n",
       " 228: 'FOODS_1_024_WI_2_evaluation',\n",
       " 229: 'FOODS_1_024_WI_3_evaluation',\n",
       " 230: 'FOODS_1_025_CA_1_evaluation',\n",
       " 231: 'FOODS_1_025_CA_2_evaluation',\n",
       " 232: 'FOODS_1_025_CA_3_evaluation',\n",
       " 233: 'FOODS_1_025_CA_4_evaluation',\n",
       " 234: 'FOODS_1_025_TX_1_evaluation',\n",
       " 235: 'FOODS_1_025_TX_2_evaluation',\n",
       " 236: 'FOODS_1_025_TX_3_evaluation',\n",
       " 237: 'FOODS_1_025_WI_1_evaluation',\n",
       " 238: 'FOODS_1_025_WI_2_evaluation',\n",
       " 239: 'FOODS_1_025_WI_3_evaluation',\n",
       " 240: 'FOODS_1_026_CA_1_evaluation',\n",
       " 241: 'FOODS_1_026_CA_2_evaluation',\n",
       " 242: 'FOODS_1_026_CA_3_evaluation',\n",
       " 243: 'FOODS_1_026_CA_4_evaluation',\n",
       " 244: 'FOODS_1_026_TX_1_evaluation',\n",
       " 245: 'FOODS_1_026_TX_2_evaluation',\n",
       " 246: 'FOODS_1_026_TX_3_evaluation',\n",
       " 247: 'FOODS_1_026_WI_1_evaluation',\n",
       " 248: 'FOODS_1_026_WI_2_evaluation',\n",
       " 249: 'FOODS_1_026_WI_3_evaluation',\n",
       " 250: 'FOODS_1_027_CA_1_evaluation',\n",
       " 251: 'FOODS_1_027_CA_2_evaluation',\n",
       " 252: 'FOODS_1_027_CA_3_evaluation',\n",
       " 253: 'FOODS_1_027_CA_4_evaluation',\n",
       " 254: 'FOODS_1_027_TX_1_evaluation',\n",
       " 255: 'FOODS_1_027_TX_2_evaluation',\n",
       " 256: 'FOODS_1_027_TX_3_evaluation',\n",
       " 257: 'FOODS_1_027_WI_1_evaluation',\n",
       " 258: 'FOODS_1_027_WI_2_evaluation',\n",
       " 259: 'FOODS_1_027_WI_3_evaluation',\n",
       " 260: 'FOODS_1_028_CA_1_evaluation',\n",
       " 261: 'FOODS_1_028_CA_2_evaluation',\n",
       " 262: 'FOODS_1_028_CA_3_evaluation',\n",
       " 263: 'FOODS_1_028_CA_4_evaluation',\n",
       " 264: 'FOODS_1_028_TX_1_evaluation',\n",
       " 265: 'FOODS_1_028_TX_2_evaluation',\n",
       " 266: 'FOODS_1_028_TX_3_evaluation',\n",
       " 267: 'FOODS_1_028_WI_1_evaluation',\n",
       " 268: 'FOODS_1_028_WI_2_evaluation',\n",
       " 269: 'FOODS_1_028_WI_3_evaluation',\n",
       " 270: 'FOODS_1_029_CA_1_evaluation',\n",
       " 271: 'FOODS_1_029_CA_2_evaluation',\n",
       " 272: 'FOODS_1_029_CA_3_evaluation',\n",
       " 273: 'FOODS_1_029_CA_4_evaluation',\n",
       " 274: 'FOODS_1_029_TX_1_evaluation',\n",
       " 275: 'FOODS_1_029_TX_2_evaluation',\n",
       " 276: 'FOODS_1_029_TX_3_evaluation',\n",
       " 277: 'FOODS_1_029_WI_1_evaluation',\n",
       " 278: 'FOODS_1_029_WI_2_evaluation',\n",
       " 279: 'FOODS_1_029_WI_3_evaluation',\n",
       " 280: 'FOODS_1_030_CA_1_evaluation',\n",
       " 281: 'FOODS_1_030_CA_2_evaluation',\n",
       " 282: 'FOODS_1_030_CA_3_evaluation',\n",
       " 283: 'FOODS_1_030_CA_4_evaluation',\n",
       " 284: 'FOODS_1_030_TX_1_evaluation',\n",
       " 285: 'FOODS_1_030_TX_2_evaluation',\n",
       " 286: 'FOODS_1_030_TX_3_evaluation',\n",
       " 287: 'FOODS_1_030_WI_1_evaluation',\n",
       " 288: 'FOODS_1_030_WI_2_evaluation',\n",
       " 289: 'FOODS_1_030_WI_3_evaluation',\n",
       " 290: 'FOODS_1_031_CA_1_evaluation',\n",
       " 291: 'FOODS_1_031_CA_2_evaluation',\n",
       " 292: 'FOODS_1_031_CA_3_evaluation',\n",
       " 293: 'FOODS_1_031_CA_4_evaluation',\n",
       " 294: 'FOODS_1_031_TX_1_evaluation',\n",
       " 295: 'FOODS_1_031_TX_2_evaluation',\n",
       " 296: 'FOODS_1_031_TX_3_evaluation',\n",
       " 297: 'FOODS_1_031_WI_1_evaluation',\n",
       " 298: 'FOODS_1_031_WI_2_evaluation',\n",
       " 299: 'FOODS_1_031_WI_3_evaluation',\n",
       " 300: 'FOODS_1_032_CA_1_evaluation',\n",
       " 301: 'FOODS_1_032_CA_2_evaluation',\n",
       " 302: 'FOODS_1_032_CA_3_evaluation',\n",
       " 303: 'FOODS_1_032_CA_4_evaluation',\n",
       " 304: 'FOODS_1_032_TX_1_evaluation',\n",
       " 305: 'FOODS_1_032_TX_2_evaluation',\n",
       " 306: 'FOODS_1_032_TX_3_evaluation',\n",
       " 307: 'FOODS_1_032_WI_1_evaluation',\n",
       " 308: 'FOODS_1_032_WI_2_evaluation',\n",
       " 309: 'FOODS_1_032_WI_3_evaluation',\n",
       " 310: 'FOODS_1_033_CA_1_evaluation',\n",
       " 311: 'FOODS_1_033_CA_2_evaluation',\n",
       " 312: 'FOODS_1_033_CA_3_evaluation',\n",
       " 313: 'FOODS_1_033_CA_4_evaluation',\n",
       " 314: 'FOODS_1_033_TX_1_evaluation',\n",
       " 315: 'FOODS_1_033_TX_2_evaluation',\n",
       " 316: 'FOODS_1_033_TX_3_evaluation',\n",
       " 317: 'FOODS_1_033_WI_1_evaluation',\n",
       " 318: 'FOODS_1_033_WI_2_evaluation',\n",
       " 319: 'FOODS_1_033_WI_3_evaluation',\n",
       " 320: 'FOODS_1_034_CA_1_evaluation',\n",
       " 321: 'FOODS_1_034_CA_2_evaluation',\n",
       " 322: 'FOODS_1_034_CA_3_evaluation',\n",
       " 323: 'FOODS_1_034_CA_4_evaluation',\n",
       " 324: 'FOODS_1_034_TX_1_evaluation',\n",
       " 325: 'FOODS_1_034_TX_2_evaluation',\n",
       " 326: 'FOODS_1_034_TX_3_evaluation',\n",
       " 327: 'FOODS_1_034_WI_1_evaluation',\n",
       " 328: 'FOODS_1_034_WI_2_evaluation',\n",
       " 329: 'FOODS_1_034_WI_3_evaluation',\n",
       " 330: 'FOODS_1_035_CA_1_evaluation',\n",
       " 331: 'FOODS_1_035_CA_2_evaluation',\n",
       " 332: 'FOODS_1_035_CA_3_evaluation',\n",
       " 333: 'FOODS_1_035_CA_4_evaluation',\n",
       " 334: 'FOODS_1_035_TX_1_evaluation',\n",
       " 335: 'FOODS_1_035_TX_2_evaluation',\n",
       " 336: 'FOODS_1_035_TX_3_evaluation',\n",
       " 337: 'FOODS_1_035_WI_1_evaluation',\n",
       " 338: 'FOODS_1_035_WI_2_evaluation',\n",
       " 339: 'FOODS_1_035_WI_3_evaluation',\n",
       " 340: 'FOODS_1_036_CA_1_evaluation',\n",
       " 341: 'FOODS_1_036_CA_2_evaluation',\n",
       " 342: 'FOODS_1_036_CA_3_evaluation',\n",
       " 343: 'FOODS_1_036_CA_4_evaluation',\n",
       " 344: 'FOODS_1_036_TX_1_evaluation',\n",
       " 345: 'FOODS_1_036_TX_2_evaluation',\n",
       " 346: 'FOODS_1_036_TX_3_evaluation',\n",
       " 347: 'FOODS_1_036_WI_1_evaluation',\n",
       " 348: 'FOODS_1_036_WI_2_evaluation',\n",
       " 349: 'FOODS_1_036_WI_3_evaluation',\n",
       " 350: 'FOODS_1_037_CA_1_evaluation',\n",
       " 351: 'FOODS_1_037_CA_2_evaluation',\n",
       " 352: 'FOODS_1_037_CA_3_evaluation',\n",
       " 353: 'FOODS_1_037_CA_4_evaluation',\n",
       " 354: 'FOODS_1_037_TX_1_evaluation',\n",
       " 355: 'FOODS_1_037_TX_2_evaluation',\n",
       " 356: 'FOODS_1_037_TX_3_evaluation',\n",
       " 357: 'FOODS_1_037_WI_1_evaluation',\n",
       " 358: 'FOODS_1_037_WI_2_evaluation',\n",
       " 359: 'FOODS_1_037_WI_3_evaluation',\n",
       " 360: 'FOODS_1_038_CA_1_evaluation',\n",
       " 361: 'FOODS_1_038_CA_2_evaluation',\n",
       " 362: 'FOODS_1_038_CA_3_evaluation',\n",
       " 363: 'FOODS_1_038_CA_4_evaluation',\n",
       " 364: 'FOODS_1_038_TX_1_evaluation',\n",
       " 365: 'FOODS_1_038_TX_2_evaluation',\n",
       " 366: 'FOODS_1_038_TX_3_evaluation',\n",
       " 367: 'FOODS_1_038_WI_1_evaluation',\n",
       " 368: 'FOODS_1_038_WI_2_evaluation',\n",
       " 369: 'FOODS_1_038_WI_3_evaluation',\n",
       " 370: 'FOODS_1_039_CA_1_evaluation',\n",
       " 371: 'FOODS_1_039_CA_2_evaluation',\n",
       " 372: 'FOODS_1_039_CA_3_evaluation',\n",
       " 373: 'FOODS_1_039_CA_4_evaluation',\n",
       " 374: 'FOODS_1_039_TX_1_evaluation',\n",
       " 375: 'FOODS_1_039_TX_2_evaluation',\n",
       " 376: 'FOODS_1_039_TX_3_evaluation',\n",
       " 377: 'FOODS_1_039_WI_1_evaluation',\n",
       " 378: 'FOODS_1_039_WI_2_evaluation',\n",
       " 379: 'FOODS_1_039_WI_3_evaluation',\n",
       " 380: 'FOODS_1_040_CA_1_evaluation',\n",
       " 381: 'FOODS_1_040_CA_2_evaluation',\n",
       " 382: 'FOODS_1_040_CA_3_evaluation',\n",
       " 383: 'FOODS_1_040_CA_4_evaluation',\n",
       " 384: 'FOODS_1_040_TX_1_evaluation',\n",
       " 385: 'FOODS_1_040_TX_2_evaluation',\n",
       " 386: 'FOODS_1_040_TX_3_evaluation',\n",
       " 387: 'FOODS_1_040_WI_1_evaluation',\n",
       " 388: 'FOODS_1_040_WI_2_evaluation',\n",
       " 389: 'FOODS_1_040_WI_3_evaluation',\n",
       " 390: 'FOODS_1_041_CA_1_evaluation',\n",
       " 391: 'FOODS_1_041_CA_2_evaluation',\n",
       " 392: 'FOODS_1_041_CA_3_evaluation',\n",
       " 393: 'FOODS_1_041_CA_4_evaluation',\n",
       " 394: 'FOODS_1_041_TX_1_evaluation',\n",
       " 395: 'FOODS_1_041_TX_2_evaluation',\n",
       " 396: 'FOODS_1_041_TX_3_evaluation',\n",
       " 397: 'FOODS_1_041_WI_1_evaluation',\n",
       " 398: 'FOODS_1_041_WI_2_evaluation',\n",
       " 399: 'FOODS_1_041_WI_3_evaluation',\n",
       " 400: 'FOODS_1_042_CA_1_evaluation',\n",
       " 401: 'FOODS_1_042_CA_2_evaluation',\n",
       " 402: 'FOODS_1_042_CA_3_evaluation',\n",
       " 403: 'FOODS_1_042_CA_4_evaluation',\n",
       " 404: 'FOODS_1_042_TX_1_evaluation',\n",
       " 405: 'FOODS_1_042_TX_2_evaluation',\n",
       " 406: 'FOODS_1_042_TX_3_evaluation',\n",
       " 407: 'FOODS_1_042_WI_1_evaluation',\n",
       " 408: 'FOODS_1_042_WI_2_evaluation',\n",
       " 409: 'FOODS_1_042_WI_3_evaluation',\n",
       " 410: 'FOODS_1_043_CA_1_evaluation',\n",
       " 411: 'FOODS_1_043_CA_2_evaluation',\n",
       " 412: 'FOODS_1_043_CA_3_evaluation',\n",
       " 413: 'FOODS_1_043_CA_4_evaluation',\n",
       " 414: 'FOODS_1_043_TX_1_evaluation',\n",
       " 415: 'FOODS_1_043_TX_2_evaluation',\n",
       " 416: 'FOODS_1_043_TX_3_evaluation',\n",
       " 417: 'FOODS_1_043_WI_1_evaluation',\n",
       " 418: 'FOODS_1_043_WI_2_evaluation',\n",
       " 419: 'FOODS_1_043_WI_3_evaluation',\n",
       " 420: 'FOODS_1_044_CA_1_evaluation',\n",
       " 421: 'FOODS_1_044_CA_2_evaluation',\n",
       " 422: 'FOODS_1_044_CA_3_evaluation',\n",
       " 423: 'FOODS_1_044_CA_4_evaluation',\n",
       " 424: 'FOODS_1_044_TX_1_evaluation',\n",
       " 425: 'FOODS_1_044_TX_2_evaluation',\n",
       " 426: 'FOODS_1_044_TX_3_evaluation',\n",
       " 427: 'FOODS_1_044_WI_1_evaluation',\n",
       " 428: 'FOODS_1_044_WI_2_evaluation',\n",
       " 429: 'FOODS_1_044_WI_3_evaluation',\n",
       " 430: 'FOODS_1_045_CA_1_evaluation',\n",
       " 431: 'FOODS_1_045_CA_2_evaluation',\n",
       " 432: 'FOODS_1_045_CA_3_evaluation',\n",
       " 433: 'FOODS_1_045_CA_4_evaluation',\n",
       " 434: 'FOODS_1_045_TX_1_evaluation',\n",
       " 435: 'FOODS_1_045_TX_2_evaluation',\n",
       " 436: 'FOODS_1_045_TX_3_evaluation',\n",
       " 437: 'FOODS_1_045_WI_1_evaluation',\n",
       " 438: 'FOODS_1_045_WI_2_evaluation',\n",
       " 439: 'FOODS_1_045_WI_3_evaluation',\n",
       " 440: 'FOODS_1_046_CA_1_evaluation',\n",
       " 441: 'FOODS_1_046_CA_2_evaluation',\n",
       " 442: 'FOODS_1_046_CA_3_evaluation',\n",
       " 443: 'FOODS_1_046_CA_4_evaluation',\n",
       " 444: 'FOODS_1_046_TX_1_evaluation',\n",
       " 445: 'FOODS_1_046_TX_2_evaluation',\n",
       " 446: 'FOODS_1_046_TX_3_evaluation',\n",
       " 447: 'FOODS_1_046_WI_1_evaluation',\n",
       " 448: 'FOODS_1_046_WI_2_evaluation',\n",
       " 449: 'FOODS_1_046_WI_3_evaluation',\n",
       " 450: 'FOODS_1_047_CA_1_evaluation',\n",
       " 451: 'FOODS_1_047_CA_2_evaluation',\n",
       " 452: 'FOODS_1_047_CA_3_evaluation',\n",
       " 453: 'FOODS_1_047_CA_4_evaluation',\n",
       " 454: 'FOODS_1_047_TX_1_evaluation',\n",
       " 455: 'FOODS_1_047_TX_2_evaluation',\n",
       " 456: 'FOODS_1_047_TX_3_evaluation',\n",
       " 457: 'FOODS_1_047_WI_1_evaluation',\n",
       " 458: 'FOODS_1_047_WI_2_evaluation',\n",
       " 459: 'FOODS_1_047_WI_3_evaluation',\n",
       " 460: 'FOODS_1_048_CA_1_evaluation',\n",
       " 461: 'FOODS_1_048_CA_2_evaluation',\n",
       " 462: 'FOODS_1_048_CA_3_evaluation',\n",
       " 463: 'FOODS_1_048_CA_4_evaluation',\n",
       " 464: 'FOODS_1_048_TX_1_evaluation',\n",
       " 465: 'FOODS_1_048_TX_2_evaluation',\n",
       " 466: 'FOODS_1_048_TX_3_evaluation',\n",
       " 467: 'FOODS_1_048_WI_1_evaluation',\n",
       " 468: 'FOODS_1_048_WI_2_evaluation',\n",
       " 469: 'FOODS_1_048_WI_3_evaluation',\n",
       " 470: 'FOODS_1_049_CA_1_evaluation',\n",
       " 471: 'FOODS_1_049_CA_2_evaluation',\n",
       " 472: 'FOODS_1_049_CA_3_evaluation',\n",
       " 473: 'FOODS_1_049_CA_4_evaluation',\n",
       " 474: 'FOODS_1_049_TX_1_evaluation',\n",
       " 475: 'FOODS_1_049_TX_2_evaluation',\n",
       " 476: 'FOODS_1_049_TX_3_evaluation',\n",
       " 477: 'FOODS_1_049_WI_1_evaluation',\n",
       " 478: 'FOODS_1_049_WI_2_evaluation',\n",
       " 479: 'FOODS_1_049_WI_3_evaluation',\n",
       " 480: 'FOODS_1_050_CA_1_evaluation',\n",
       " 481: 'FOODS_1_050_CA_2_evaluation',\n",
       " 482: 'FOODS_1_050_CA_3_evaluation',\n",
       " 483: 'FOODS_1_050_CA_4_evaluation',\n",
       " 484: 'FOODS_1_050_TX_1_evaluation',\n",
       " 485: 'FOODS_1_050_TX_2_evaluation',\n",
       " 486: 'FOODS_1_050_TX_3_evaluation',\n",
       " 487: 'FOODS_1_050_WI_1_evaluation',\n",
       " 488: 'FOODS_1_050_WI_2_evaluation',\n",
       " 489: 'FOODS_1_050_WI_3_evaluation',\n",
       " 490: 'FOODS_1_051_CA_1_evaluation',\n",
       " 491: 'FOODS_1_051_CA_2_evaluation',\n",
       " 492: 'FOODS_1_051_CA_3_evaluation',\n",
       " 493: 'FOODS_1_051_CA_4_evaluation',\n",
       " 494: 'FOODS_1_051_TX_1_evaluation',\n",
       " 495: 'FOODS_1_051_TX_2_evaluation',\n",
       " 496: 'FOODS_1_051_TX_3_evaluation',\n",
       " 497: 'FOODS_1_051_WI_1_evaluation',\n",
       " 498: 'FOODS_1_051_WI_2_evaluation',\n",
       " 499: 'FOODS_1_051_WI_3_evaluation',\n",
       " 500: 'FOODS_1_052_CA_1_evaluation',\n",
       " 501: 'FOODS_1_052_CA_2_evaluation',\n",
       " 502: 'FOODS_1_052_CA_3_evaluation',\n",
       " 503: 'FOODS_1_052_CA_4_evaluation',\n",
       " 504: 'FOODS_1_052_TX_1_evaluation',\n",
       " 505: 'FOODS_1_052_TX_2_evaluation',\n",
       " 506: 'FOODS_1_052_TX_3_evaluation',\n",
       " 507: 'FOODS_1_052_WI_1_evaluation',\n",
       " 508: 'FOODS_1_052_WI_2_evaluation',\n",
       " 509: 'FOODS_1_052_WI_3_evaluation',\n",
       " 510: 'FOODS_1_053_CA_1_evaluation',\n",
       " 511: 'FOODS_1_053_CA_2_evaluation',\n",
       " 512: 'FOODS_1_053_CA_3_evaluation',\n",
       " 513: 'FOODS_1_053_CA_4_evaluation',\n",
       " 514: 'FOODS_1_053_TX_1_evaluation',\n",
       " 515: 'FOODS_1_053_TX_2_evaluation',\n",
       " 516: 'FOODS_1_053_TX_3_evaluation',\n",
       " 517: 'FOODS_1_053_WI_1_evaluation',\n",
       " 518: 'FOODS_1_053_WI_2_evaluation',\n",
       " 519: 'FOODS_1_053_WI_3_evaluation',\n",
       " 520: 'FOODS_1_054_CA_1_evaluation',\n",
       " 521: 'FOODS_1_054_CA_2_evaluation',\n",
       " 522: 'FOODS_1_054_CA_3_evaluation',\n",
       " 523: 'FOODS_1_054_CA_4_evaluation',\n",
       " 524: 'FOODS_1_054_TX_1_evaluation',\n",
       " 525: 'FOODS_1_054_TX_2_evaluation',\n",
       " 526: 'FOODS_1_054_TX_3_evaluation',\n",
       " 527: 'FOODS_1_054_WI_1_evaluation',\n",
       " 528: 'FOODS_1_054_WI_2_evaluation',\n",
       " 529: 'FOODS_1_054_WI_3_evaluation',\n",
       " 530: 'FOODS_1_055_CA_1_evaluation',\n",
       " 531: 'FOODS_1_055_CA_2_evaluation',\n",
       " 532: 'FOODS_1_055_CA_3_evaluation',\n",
       " 533: 'FOODS_1_055_CA_4_evaluation',\n",
       " 534: 'FOODS_1_055_TX_1_evaluation',\n",
       " 535: 'FOODS_1_055_TX_2_evaluation',\n",
       " 536: 'FOODS_1_055_TX_3_evaluation',\n",
       " 537: 'FOODS_1_055_WI_1_evaluation',\n",
       " 538: 'FOODS_1_055_WI_2_evaluation',\n",
       " 539: 'FOODS_1_055_WI_3_evaluation',\n",
       " 540: 'FOODS_1_056_CA_1_evaluation',\n",
       " 541: 'FOODS_1_056_CA_2_evaluation',\n",
       " 542: 'FOODS_1_056_CA_3_evaluation',\n",
       " 543: 'FOODS_1_056_CA_4_evaluation',\n",
       " 544: 'FOODS_1_056_TX_1_evaluation',\n",
       " 545: 'FOODS_1_056_TX_2_evaluation',\n",
       " 546: 'FOODS_1_056_TX_3_evaluation',\n",
       " 547: 'FOODS_1_056_WI_1_evaluation',\n",
       " 548: 'FOODS_1_056_WI_2_evaluation',\n",
       " 549: 'FOODS_1_056_WI_3_evaluation',\n",
       " 550: 'FOODS_1_057_CA_1_evaluation',\n",
       " 551: 'FOODS_1_057_CA_2_evaluation',\n",
       " 552: 'FOODS_1_057_CA_3_evaluation',\n",
       " 553: 'FOODS_1_057_CA_4_evaluation',\n",
       " 554: 'FOODS_1_057_TX_1_evaluation',\n",
       " 555: 'FOODS_1_057_TX_2_evaluation',\n",
       " 556: 'FOODS_1_057_TX_3_evaluation',\n",
       " 557: 'FOODS_1_057_WI_1_evaluation',\n",
       " 558: 'FOODS_1_057_WI_2_evaluation',\n",
       " 559: 'FOODS_1_057_WI_3_evaluation',\n",
       " 560: 'FOODS_1_058_CA_1_evaluation',\n",
       " 561: 'FOODS_1_058_CA_2_evaluation',\n",
       " 562: 'FOODS_1_058_CA_3_evaluation',\n",
       " 563: 'FOODS_1_058_CA_4_evaluation',\n",
       " 564: 'FOODS_1_058_TX_1_evaluation',\n",
       " 565: 'FOODS_1_058_TX_2_evaluation',\n",
       " 566: 'FOODS_1_058_TX_3_evaluation',\n",
       " 567: 'FOODS_1_058_WI_1_evaluation',\n",
       " 568: 'FOODS_1_058_WI_2_evaluation',\n",
       " 569: 'FOODS_1_058_WI_3_evaluation',\n",
       " 570: 'FOODS_1_059_CA_1_evaluation',\n",
       " 571: 'FOODS_1_059_CA_2_evaluation',\n",
       " 572: 'FOODS_1_059_CA_3_evaluation',\n",
       " 573: 'FOODS_1_059_CA_4_evaluation',\n",
       " 574: 'FOODS_1_059_TX_1_evaluation',\n",
       " 575: 'FOODS_1_059_TX_2_evaluation',\n",
       " 576: 'FOODS_1_059_TX_3_evaluation',\n",
       " 577: 'FOODS_1_059_WI_1_evaluation',\n",
       " 578: 'FOODS_1_059_WI_2_evaluation',\n",
       " 579: 'FOODS_1_059_WI_3_evaluation',\n",
       " 580: 'FOODS_1_060_CA_1_evaluation',\n",
       " 581: 'FOODS_1_060_CA_2_evaluation',\n",
       " 582: 'FOODS_1_060_CA_3_evaluation',\n",
       " 583: 'FOODS_1_060_CA_4_evaluation',\n",
       " 584: 'FOODS_1_060_TX_1_evaluation',\n",
       " 585: 'FOODS_1_060_TX_2_evaluation',\n",
       " 586: 'FOODS_1_060_TX_3_evaluation',\n",
       " 587: 'FOODS_1_060_WI_1_evaluation',\n",
       " 588: 'FOODS_1_060_WI_2_evaluation',\n",
       " 589: 'FOODS_1_060_WI_3_evaluation',\n",
       " 590: 'FOODS_1_061_CA_1_evaluation',\n",
       " 591: 'FOODS_1_061_CA_2_evaluation',\n",
       " 592: 'FOODS_1_061_CA_3_evaluation',\n",
       " 593: 'FOODS_1_061_CA_4_evaluation',\n",
       " 594: 'FOODS_1_061_TX_1_evaluation',\n",
       " 595: 'FOODS_1_061_TX_2_evaluation',\n",
       " 596: 'FOODS_1_061_TX_3_evaluation',\n",
       " 597: 'FOODS_1_061_WI_1_evaluation',\n",
       " 598: 'FOODS_1_061_WI_2_evaluation',\n",
       " 599: 'FOODS_1_061_WI_3_evaluation',\n",
       " 600: 'FOODS_1_062_CA_1_evaluation',\n",
       " 601: 'FOODS_1_062_CA_2_evaluation',\n",
       " 602: 'FOODS_1_062_CA_3_evaluation',\n",
       " 603: 'FOODS_1_062_CA_4_evaluation',\n",
       " 604: 'FOODS_1_062_TX_1_evaluation',\n",
       " 605: 'FOODS_1_062_TX_2_evaluation',\n",
       " 606: 'FOODS_1_062_TX_3_evaluation',\n",
       " 607: 'FOODS_1_062_WI_1_evaluation',\n",
       " 608: 'FOODS_1_062_WI_2_evaluation',\n",
       " 609: 'FOODS_1_062_WI_3_evaluation',\n",
       " 610: 'FOODS_1_063_CA_1_evaluation',\n",
       " 611: 'FOODS_1_063_CA_2_evaluation',\n",
       " 612: 'FOODS_1_063_CA_3_evaluation',\n",
       " 613: 'FOODS_1_063_CA_4_evaluation',\n",
       " 614: 'FOODS_1_063_TX_1_evaluation',\n",
       " 615: 'FOODS_1_063_TX_2_evaluation',\n",
       " 616: 'FOODS_1_063_TX_3_evaluation',\n",
       " 617: 'FOODS_1_063_WI_1_evaluation',\n",
       " 618: 'FOODS_1_063_WI_2_evaluation',\n",
       " 619: 'FOODS_1_063_WI_3_evaluation',\n",
       " 620: 'FOODS_1_064_CA_1_evaluation',\n",
       " 621: 'FOODS_1_064_CA_2_evaluation',\n",
       " 622: 'FOODS_1_064_CA_3_evaluation',\n",
       " 623: 'FOODS_1_064_CA_4_evaluation',\n",
       " 624: 'FOODS_1_064_TX_1_evaluation',\n",
       " 625: 'FOODS_1_064_TX_2_evaluation',\n",
       " 626: 'FOODS_1_064_TX_3_evaluation',\n",
       " 627: 'FOODS_1_064_WI_1_evaluation',\n",
       " 628: 'FOODS_1_064_WI_2_evaluation',\n",
       " 629: 'FOODS_1_064_WI_3_evaluation',\n",
       " 630: 'FOODS_1_065_CA_1_evaluation',\n",
       " 631: 'FOODS_1_065_CA_2_evaluation',\n",
       " 632: 'FOODS_1_065_CA_3_evaluation',\n",
       " 633: 'FOODS_1_065_CA_4_evaluation',\n",
       " 634: 'FOODS_1_065_TX_1_evaluation',\n",
       " 635: 'FOODS_1_065_TX_2_evaluation',\n",
       " 636: 'FOODS_1_065_TX_3_evaluation',\n",
       " 637: 'FOODS_1_065_WI_1_evaluation',\n",
       " 638: 'FOODS_1_065_WI_2_evaluation',\n",
       " 639: 'FOODS_1_065_WI_3_evaluation',\n",
       " 640: 'FOODS_1_066_CA_1_evaluation',\n",
       " 641: 'FOODS_1_066_CA_2_evaluation',\n",
       " 642: 'FOODS_1_066_CA_3_evaluation',\n",
       " 643: 'FOODS_1_066_CA_4_evaluation',\n",
       " 644: 'FOODS_1_066_TX_1_evaluation',\n",
       " 645: 'FOODS_1_066_TX_2_evaluation',\n",
       " 646: 'FOODS_1_066_TX_3_evaluation',\n",
       " 647: 'FOODS_1_066_WI_1_evaluation',\n",
       " 648: 'FOODS_1_066_WI_2_evaluation',\n",
       " 649: 'FOODS_1_066_WI_3_evaluation',\n",
       " 650: 'FOODS_1_067_CA_1_evaluation',\n",
       " 651: 'FOODS_1_067_CA_2_evaluation',\n",
       " 652: 'FOODS_1_067_CA_3_evaluation',\n",
       " 653: 'FOODS_1_067_CA_4_evaluation',\n",
       " 654: 'FOODS_1_067_TX_1_evaluation',\n",
       " 655: 'FOODS_1_067_TX_2_evaluation',\n",
       " 656: 'FOODS_1_067_TX_3_evaluation',\n",
       " 657: 'FOODS_1_067_WI_1_evaluation',\n",
       " 658: 'FOODS_1_067_WI_2_evaluation',\n",
       " 659: 'FOODS_1_067_WI_3_evaluation',\n",
       " 660: 'FOODS_1_068_CA_1_evaluation',\n",
       " 661: 'FOODS_1_068_CA_2_evaluation',\n",
       " 662: 'FOODS_1_068_CA_3_evaluation',\n",
       " 663: 'FOODS_1_068_CA_4_evaluation',\n",
       " 664: 'FOODS_1_068_TX_1_evaluation',\n",
       " 665: 'FOODS_1_068_TX_2_evaluation',\n",
       " 666: 'FOODS_1_068_TX_3_evaluation',\n",
       " 667: 'FOODS_1_068_WI_1_evaluation',\n",
       " 668: 'FOODS_1_068_WI_2_evaluation',\n",
       " 669: 'FOODS_1_068_WI_3_evaluation',\n",
       " 670: 'FOODS_1_069_CA_1_evaluation',\n",
       " 671: 'FOODS_1_069_CA_2_evaluation',\n",
       " 672: 'FOODS_1_069_CA_3_evaluation',\n",
       " 673: 'FOODS_1_069_CA_4_evaluation',\n",
       " 674: 'FOODS_1_069_TX_1_evaluation',\n",
       " 675: 'FOODS_1_069_TX_2_evaluation',\n",
       " 676: 'FOODS_1_069_TX_3_evaluation',\n",
       " 677: 'FOODS_1_069_WI_1_evaluation',\n",
       " 678: 'FOODS_1_069_WI_2_evaluation',\n",
       " 679: 'FOODS_1_069_WI_3_evaluation',\n",
       " 680: 'FOODS_1_070_CA_1_evaluation',\n",
       " 681: 'FOODS_1_070_CA_2_evaluation',\n",
       " 682: 'FOODS_1_070_CA_3_evaluation',\n",
       " 683: 'FOODS_1_070_CA_4_evaluation',\n",
       " 684: 'FOODS_1_070_TX_1_evaluation',\n",
       " 685: 'FOODS_1_070_TX_2_evaluation',\n",
       " 686: 'FOODS_1_070_TX_3_evaluation',\n",
       " 687: 'FOODS_1_070_WI_1_evaluation',\n",
       " 688: 'FOODS_1_070_WI_2_evaluation',\n",
       " 689: 'FOODS_1_070_WI_3_evaluation',\n",
       " 690: 'FOODS_1_071_CA_1_evaluation',\n",
       " 691: 'FOODS_1_071_CA_2_evaluation',\n",
       " 692: 'FOODS_1_071_CA_3_evaluation',\n",
       " 693: 'FOODS_1_071_CA_4_evaluation',\n",
       " 694: 'FOODS_1_071_TX_1_evaluation',\n",
       " 695: 'FOODS_1_071_TX_2_evaluation',\n",
       " 696: 'FOODS_1_071_TX_3_evaluation',\n",
       " 697: 'FOODS_1_071_WI_1_evaluation',\n",
       " 698: 'FOODS_1_071_WI_2_evaluation',\n",
       " 699: 'FOODS_1_071_WI_3_evaluation',\n",
       " 700: 'FOODS_1_072_CA_1_evaluation',\n",
       " 701: 'FOODS_1_072_CA_2_evaluation',\n",
       " 702: 'FOODS_1_072_CA_3_evaluation',\n",
       " 703: 'FOODS_1_072_CA_4_evaluation',\n",
       " 704: 'FOODS_1_072_TX_1_evaluation',\n",
       " 705: 'FOODS_1_072_TX_2_evaluation',\n",
       " 706: 'FOODS_1_072_TX_3_evaluation',\n",
       " 707: 'FOODS_1_072_WI_1_evaluation',\n",
       " 708: 'FOODS_1_072_WI_2_evaluation',\n",
       " 709: 'FOODS_1_072_WI_3_evaluation',\n",
       " 710: 'FOODS_1_073_CA_1_evaluation',\n",
       " 711: 'FOODS_1_073_CA_2_evaluation',\n",
       " 712: 'FOODS_1_073_CA_3_evaluation',\n",
       " 713: 'FOODS_1_073_CA_4_evaluation',\n",
       " 714: 'FOODS_1_073_TX_1_evaluation',\n",
       " 715: 'FOODS_1_073_TX_2_evaluation',\n",
       " 716: 'FOODS_1_073_TX_3_evaluation',\n",
       " 717: 'FOODS_1_073_WI_1_evaluation',\n",
       " 718: 'FOODS_1_073_WI_2_evaluation',\n",
       " 719: 'FOODS_1_073_WI_3_evaluation',\n",
       " 720: 'FOODS_1_074_CA_1_evaluation',\n",
       " 721: 'FOODS_1_074_CA_2_evaluation',\n",
       " 722: 'FOODS_1_074_CA_3_evaluation',\n",
       " 723: 'FOODS_1_074_CA_4_evaluation',\n",
       " 724: 'FOODS_1_074_TX_1_evaluation',\n",
       " 725: 'FOODS_1_074_TX_2_evaluation',\n",
       " 726: 'FOODS_1_074_TX_3_evaluation',\n",
       " 727: 'FOODS_1_074_WI_1_evaluation',\n",
       " 728: 'FOODS_1_074_WI_2_evaluation',\n",
       " 729: 'FOODS_1_074_WI_3_evaluation',\n",
       " 730: 'FOODS_1_075_CA_1_evaluation',\n",
       " 731: 'FOODS_1_075_CA_2_evaluation',\n",
       " 732: 'FOODS_1_075_CA_3_evaluation',\n",
       " 733: 'FOODS_1_075_CA_4_evaluation',\n",
       " 734: 'FOODS_1_075_TX_1_evaluation',\n",
       " 735: 'FOODS_1_075_TX_2_evaluation',\n",
       " 736: 'FOODS_1_075_TX_3_evaluation',\n",
       " 737: 'FOODS_1_075_WI_1_evaluation',\n",
       " 738: 'FOODS_1_075_WI_2_evaluation',\n",
       " 739: 'FOODS_1_075_WI_3_evaluation',\n",
       " 740: 'FOODS_1_076_CA_1_evaluation',\n",
       " 741: 'FOODS_1_076_CA_2_evaluation',\n",
       " 742: 'FOODS_1_076_CA_3_evaluation',\n",
       " 743: 'FOODS_1_076_CA_4_evaluation',\n",
       " 744: 'FOODS_1_076_TX_1_evaluation',\n",
       " 745: 'FOODS_1_076_TX_2_evaluation',\n",
       " 746: 'FOODS_1_076_TX_3_evaluation',\n",
       " 747: 'FOODS_1_076_WI_1_evaluation',\n",
       " 748: 'FOODS_1_076_WI_2_evaluation',\n",
       " 749: 'FOODS_1_076_WI_3_evaluation',\n",
       " 750: 'FOODS_1_077_CA_1_evaluation',\n",
       " 751: 'FOODS_1_077_CA_2_evaluation',\n",
       " 752: 'FOODS_1_077_CA_3_evaluation',\n",
       " 753: 'FOODS_1_077_CA_4_evaluation',\n",
       " 754: 'FOODS_1_077_TX_1_evaluation',\n",
       " 755: 'FOODS_1_077_TX_2_evaluation',\n",
       " 756: 'FOODS_1_077_TX_3_evaluation',\n",
       " 757: 'FOODS_1_077_WI_1_evaluation',\n",
       " 758: 'FOODS_1_077_WI_2_evaluation',\n",
       " 759: 'FOODS_1_077_WI_3_evaluation',\n",
       " 760: 'FOODS_1_078_CA_1_evaluation',\n",
       " 761: 'FOODS_1_078_CA_2_evaluation',\n",
       " 762: 'FOODS_1_078_CA_3_evaluation',\n",
       " 763: 'FOODS_1_078_CA_4_evaluation',\n",
       " 764: 'FOODS_1_078_TX_1_evaluation',\n",
       " 765: 'FOODS_1_078_TX_2_evaluation',\n",
       " 766: 'FOODS_1_078_TX_3_evaluation',\n",
       " 767: 'FOODS_1_078_WI_1_evaluation',\n",
       " 768: 'FOODS_1_078_WI_2_evaluation',\n",
       " 769: 'FOODS_1_078_WI_3_evaluation',\n",
       " 770: 'FOODS_1_079_CA_1_evaluation',\n",
       " 771: 'FOODS_1_079_CA_2_evaluation',\n",
       " 772: 'FOODS_1_079_CA_3_evaluation',\n",
       " 773: 'FOODS_1_079_CA_4_evaluation',\n",
       " 774: 'FOODS_1_079_TX_1_evaluation',\n",
       " 775: 'FOODS_1_079_TX_2_evaluation',\n",
       " 776: 'FOODS_1_079_TX_3_evaluation',\n",
       " 777: 'FOODS_1_079_WI_1_evaluation',\n",
       " 778: 'FOODS_1_079_WI_2_evaluation',\n",
       " 779: 'FOODS_1_079_WI_3_evaluation',\n",
       " 780: 'FOODS_1_080_CA_1_evaluation',\n",
       " 781: 'FOODS_1_080_CA_2_evaluation',\n",
       " 782: 'FOODS_1_080_CA_3_evaluation',\n",
       " 783: 'FOODS_1_080_CA_4_evaluation',\n",
       " 784: 'FOODS_1_080_TX_1_evaluation',\n",
       " 785: 'FOODS_1_080_TX_2_evaluation',\n",
       " 786: 'FOODS_1_080_TX_3_evaluation',\n",
       " 787: 'FOODS_1_080_WI_1_evaluation',\n",
       " 788: 'FOODS_1_080_WI_2_evaluation',\n",
       " 789: 'FOODS_1_080_WI_3_evaluation',\n",
       " 790: 'FOODS_1_081_CA_1_evaluation',\n",
       " 791: 'FOODS_1_081_CA_2_evaluation',\n",
       " 792: 'FOODS_1_081_CA_3_evaluation',\n",
       " 793: 'FOODS_1_081_CA_4_evaluation',\n",
       " 794: 'FOODS_1_081_TX_1_evaluation',\n",
       " 795: 'FOODS_1_081_TX_2_evaluation',\n",
       " 796: 'FOODS_1_081_TX_3_evaluation',\n",
       " 797: 'FOODS_1_081_WI_1_evaluation',\n",
       " 798: 'FOODS_1_081_WI_2_evaluation',\n",
       " 799: 'FOODS_1_081_WI_3_evaluation',\n",
       " 800: 'FOODS_1_082_CA_1_evaluation',\n",
       " 801: 'FOODS_1_082_CA_2_evaluation',\n",
       " 802: 'FOODS_1_082_CA_3_evaluation',\n",
       " 803: 'FOODS_1_082_CA_4_evaluation',\n",
       " 804: 'FOODS_1_082_TX_1_evaluation',\n",
       " 805: 'FOODS_1_082_TX_2_evaluation',\n",
       " 806: 'FOODS_1_082_TX_3_evaluation',\n",
       " 807: 'FOODS_1_082_WI_1_evaluation',\n",
       " 808: 'FOODS_1_082_WI_2_evaluation',\n",
       " 809: 'FOODS_1_082_WI_3_evaluation',\n",
       " 810: 'FOODS_1_083_CA_1_evaluation',\n",
       " 811: 'FOODS_1_083_CA_2_evaluation',\n",
       " 812: 'FOODS_1_083_CA_3_evaluation',\n",
       " 813: 'FOODS_1_083_CA_4_evaluation',\n",
       " 814: 'FOODS_1_083_TX_1_evaluation',\n",
       " 815: 'FOODS_1_083_TX_2_evaluation',\n",
       " 816: 'FOODS_1_083_TX_3_evaluation',\n",
       " 817: 'FOODS_1_083_WI_1_evaluation',\n",
       " 818: 'FOODS_1_083_WI_2_evaluation',\n",
       " 819: 'FOODS_1_083_WI_3_evaluation',\n",
       " 820: 'FOODS_1_084_CA_1_evaluation',\n",
       " 821: 'FOODS_1_084_CA_2_evaluation',\n",
       " 822: 'FOODS_1_084_CA_3_evaluation',\n",
       " 823: 'FOODS_1_084_CA_4_evaluation',\n",
       " 824: 'FOODS_1_084_TX_1_evaluation',\n",
       " 825: 'FOODS_1_084_TX_2_evaluation',\n",
       " 826: 'FOODS_1_084_TX_3_evaluation',\n",
       " 827: 'FOODS_1_084_WI_1_evaluation',\n",
       " 828: 'FOODS_1_084_WI_2_evaluation',\n",
       " 829: 'FOODS_1_084_WI_3_evaluation',\n",
       " 830: 'FOODS_1_085_CA_1_evaluation',\n",
       " 831: 'FOODS_1_085_CA_2_evaluation',\n",
       " 832: 'FOODS_1_085_CA_3_evaluation',\n",
       " 833: 'FOODS_1_085_CA_4_evaluation',\n",
       " 834: 'FOODS_1_085_TX_1_evaluation',\n",
       " 835: 'FOODS_1_085_TX_2_evaluation',\n",
       " 836: 'FOODS_1_085_TX_3_evaluation',\n",
       " 837: 'FOODS_1_085_WI_1_evaluation',\n",
       " 838: 'FOODS_1_085_WI_2_evaluation',\n",
       " 839: 'FOODS_1_085_WI_3_evaluation',\n",
       " 840: 'FOODS_1_086_CA_1_evaluation',\n",
       " 841: 'FOODS_1_086_CA_2_evaluation',\n",
       " 842: 'FOODS_1_086_CA_3_evaluation',\n",
       " 843: 'FOODS_1_086_CA_4_evaluation',\n",
       " 844: 'FOODS_1_086_TX_1_evaluation',\n",
       " 845: 'FOODS_1_086_TX_2_evaluation',\n",
       " 846: 'FOODS_1_086_TX_3_evaluation',\n",
       " 847: 'FOODS_1_086_WI_1_evaluation',\n",
       " 848: 'FOODS_1_086_WI_2_evaluation',\n",
       " 849: 'FOODS_1_086_WI_3_evaluation',\n",
       " 850: 'FOODS_1_087_CA_1_evaluation',\n",
       " 851: 'FOODS_1_087_CA_2_evaluation',\n",
       " 852: 'FOODS_1_087_CA_3_evaluation',\n",
       " 853: 'FOODS_1_087_CA_4_evaluation',\n",
       " 854: 'FOODS_1_087_TX_1_evaluation',\n",
       " 855: 'FOODS_1_087_TX_2_evaluation',\n",
       " 856: 'FOODS_1_087_TX_3_evaluation',\n",
       " 857: 'FOODS_1_087_WI_1_evaluation',\n",
       " 858: 'FOODS_1_087_WI_2_evaluation',\n",
       " 859: 'FOODS_1_087_WI_3_evaluation',\n",
       " 860: 'FOODS_1_088_CA_1_evaluation',\n",
       " 861: 'FOODS_1_088_CA_2_evaluation',\n",
       " 862: 'FOODS_1_088_CA_3_evaluation',\n",
       " 863: 'FOODS_1_088_CA_4_evaluation',\n",
       " 864: 'FOODS_1_088_TX_1_evaluation',\n",
       " 865: 'FOODS_1_088_TX_2_evaluation',\n",
       " 866: 'FOODS_1_088_TX_3_evaluation',\n",
       " 867: 'FOODS_1_088_WI_1_evaluation',\n",
       " 868: 'FOODS_1_088_WI_2_evaluation',\n",
       " 869: 'FOODS_1_088_WI_3_evaluation',\n",
       " 870: 'FOODS_1_089_CA_1_evaluation',\n",
       " 871: 'FOODS_1_089_CA_2_evaluation',\n",
       " 872: 'FOODS_1_089_CA_3_evaluation',\n",
       " 873: 'FOODS_1_089_CA_4_evaluation',\n",
       " 874: 'FOODS_1_089_TX_1_evaluation',\n",
       " 875: 'FOODS_1_089_TX_2_evaluation',\n",
       " 876: 'FOODS_1_089_TX_3_evaluation',\n",
       " 877: 'FOODS_1_089_WI_1_evaluation',\n",
       " 878: 'FOODS_1_089_WI_2_evaluation',\n",
       " 879: 'FOODS_1_089_WI_3_evaluation',\n",
       " 880: 'FOODS_1_090_CA_1_evaluation',\n",
       " 881: 'FOODS_1_090_CA_2_evaluation',\n",
       " 882: 'FOODS_1_090_CA_3_evaluation',\n",
       " 883: 'FOODS_1_090_CA_4_evaluation',\n",
       " 884: 'FOODS_1_090_TX_1_evaluation',\n",
       " 885: 'FOODS_1_090_TX_2_evaluation',\n",
       " 886: 'FOODS_1_090_TX_3_evaluation',\n",
       " 887: 'FOODS_1_090_WI_1_evaluation',\n",
       " 888: 'FOODS_1_090_WI_2_evaluation',\n",
       " 889: 'FOODS_1_090_WI_3_evaluation',\n",
       " 890: 'FOODS_1_091_CA_1_evaluation',\n",
       " 891: 'FOODS_1_091_CA_2_evaluation',\n",
       " 892: 'FOODS_1_091_CA_3_evaluation',\n",
       " 893: 'FOODS_1_091_CA_4_evaluation',\n",
       " 894: 'FOODS_1_091_TX_1_evaluation',\n",
       " 895: 'FOODS_1_091_TX_2_evaluation',\n",
       " 896: 'FOODS_1_091_TX_3_evaluation',\n",
       " 897: 'FOODS_1_091_WI_1_evaluation',\n",
       " 898: 'FOODS_1_091_WI_2_evaluation',\n",
       " 899: 'FOODS_1_091_WI_3_evaluation',\n",
       " 900: 'FOODS_1_092_CA_1_evaluation',\n",
       " 901: 'FOODS_1_092_CA_2_evaluation',\n",
       " 902: 'FOODS_1_092_CA_3_evaluation',\n",
       " 903: 'FOODS_1_092_CA_4_evaluation',\n",
       " 904: 'FOODS_1_092_TX_1_evaluation',\n",
       " 905: 'FOODS_1_092_TX_2_evaluation',\n",
       " 906: 'FOODS_1_092_TX_3_evaluation',\n",
       " 907: 'FOODS_1_092_WI_1_evaluation',\n",
       " 908: 'FOODS_1_092_WI_2_evaluation',\n",
       " 909: 'FOODS_1_092_WI_3_evaluation',\n",
       " 910: 'FOODS_1_093_CA_1_evaluation',\n",
       " 911: 'FOODS_1_093_CA_2_evaluation',\n",
       " 912: 'FOODS_1_093_CA_3_evaluation',\n",
       " 913: 'FOODS_1_093_CA_4_evaluation',\n",
       " 914: 'FOODS_1_093_TX_1_evaluation',\n",
       " 915: 'FOODS_1_093_TX_2_evaluation',\n",
       " 916: 'FOODS_1_093_TX_3_evaluation',\n",
       " 917: 'FOODS_1_093_WI_1_evaluation',\n",
       " 918: 'FOODS_1_093_WI_2_evaluation',\n",
       " 919: 'FOODS_1_093_WI_3_evaluation',\n",
       " 920: 'FOODS_1_094_CA_1_evaluation',\n",
       " 921: 'FOODS_1_094_CA_2_evaluation',\n",
       " 922: 'FOODS_1_094_CA_3_evaluation',\n",
       " 923: 'FOODS_1_094_CA_4_evaluation',\n",
       " 924: 'FOODS_1_094_TX_1_evaluation',\n",
       " 925: 'FOODS_1_094_TX_2_evaluation',\n",
       " 926: 'FOODS_1_094_TX_3_evaluation',\n",
       " 927: 'FOODS_1_094_WI_1_evaluation',\n",
       " 928: 'FOODS_1_094_WI_2_evaluation',\n",
       " 929: 'FOODS_1_094_WI_3_evaluation',\n",
       " 930: 'FOODS_1_095_CA_1_evaluation',\n",
       " 931: 'FOODS_1_095_CA_2_evaluation',\n",
       " 932: 'FOODS_1_095_CA_3_evaluation',\n",
       " 933: 'FOODS_1_095_CA_4_evaluation',\n",
       " 934: 'FOODS_1_095_TX_1_evaluation',\n",
       " 935: 'FOODS_1_095_TX_2_evaluation',\n",
       " 936: 'FOODS_1_095_TX_3_evaluation',\n",
       " 937: 'FOODS_1_095_WI_1_evaluation',\n",
       " 938: 'FOODS_1_095_WI_2_evaluation',\n",
       " 939: 'FOODS_1_095_WI_3_evaluation',\n",
       " 940: 'FOODS_1_096_CA_1_evaluation',\n",
       " 941: 'FOODS_1_096_CA_2_evaluation',\n",
       " 942: 'FOODS_1_096_CA_3_evaluation',\n",
       " 943: 'FOODS_1_096_CA_4_evaluation',\n",
       " 944: 'FOODS_1_096_TX_1_evaluation',\n",
       " 945: 'FOODS_1_096_TX_2_evaluation',\n",
       " 946: 'FOODS_1_096_TX_3_evaluation',\n",
       " 947: 'FOODS_1_096_WI_1_evaluation',\n",
       " 948: 'FOODS_1_096_WI_2_evaluation',\n",
       " 949: 'FOODS_1_096_WI_3_evaluation',\n",
       " 950: 'FOODS_1_097_CA_1_evaluation',\n",
       " 951: 'FOODS_1_097_CA_2_evaluation',\n",
       " 952: 'FOODS_1_097_CA_3_evaluation',\n",
       " 953: 'FOODS_1_097_CA_4_evaluation',\n",
       " 954: 'FOODS_1_097_TX_1_evaluation',\n",
       " 955: 'FOODS_1_097_TX_2_evaluation',\n",
       " 956: 'FOODS_1_097_TX_3_evaluation',\n",
       " 957: 'FOODS_1_097_WI_1_evaluation',\n",
       " 958: 'FOODS_1_097_WI_2_evaluation',\n",
       " 959: 'FOODS_1_097_WI_3_evaluation',\n",
       " 960: 'FOODS_1_098_CA_1_evaluation',\n",
       " 961: 'FOODS_1_098_CA_2_evaluation',\n",
       " 962: 'FOODS_1_098_CA_3_evaluation',\n",
       " 963: 'FOODS_1_098_CA_4_evaluation',\n",
       " 964: 'FOODS_1_098_TX_1_evaluation',\n",
       " 965: 'FOODS_1_098_TX_2_evaluation',\n",
       " 966: 'FOODS_1_098_TX_3_evaluation',\n",
       " 967: 'FOODS_1_098_WI_1_evaluation',\n",
       " 968: 'FOODS_1_098_WI_2_evaluation',\n",
       " 969: 'FOODS_1_098_WI_3_evaluation',\n",
       " 970: 'FOODS_1_099_CA_1_evaluation',\n",
       " 971: 'FOODS_1_099_CA_2_evaluation',\n",
       " 972: 'FOODS_1_099_CA_3_evaluation',\n",
       " 973: 'FOODS_1_099_CA_4_evaluation',\n",
       " 974: 'FOODS_1_099_TX_1_evaluation',\n",
       " 975: 'FOODS_1_099_TX_2_evaluation',\n",
       " 976: 'FOODS_1_099_TX_3_evaluation',\n",
       " 977: 'FOODS_1_099_WI_1_evaluation',\n",
       " 978: 'FOODS_1_099_WI_2_evaluation',\n",
       " 979: 'FOODS_1_099_WI_3_evaluation',\n",
       " 980: 'FOODS_1_101_CA_1_evaluation',\n",
       " 981: 'FOODS_1_101_CA_2_evaluation',\n",
       " 982: 'FOODS_1_101_CA_3_evaluation',\n",
       " 983: 'FOODS_1_101_CA_4_evaluation',\n",
       " 984: 'FOODS_1_101_TX_1_evaluation',\n",
       " 985: 'FOODS_1_101_TX_2_evaluation',\n",
       " 986: 'FOODS_1_101_TX_3_evaluation',\n",
       " 987: 'FOODS_1_101_WI_1_evaluation',\n",
       " 988: 'FOODS_1_101_WI_2_evaluation',\n",
       " 989: 'FOODS_1_101_WI_3_evaluation',\n",
       " 990: 'FOODS_1_102_CA_1_evaluation',\n",
       " 991: 'FOODS_1_102_CA_2_evaluation',\n",
       " 992: 'FOODS_1_102_CA_3_evaluation',\n",
       " 993: 'FOODS_1_102_CA_4_evaluation',\n",
       " 994: 'FOODS_1_102_TX_1_evaluation',\n",
       " 995: 'FOODS_1_102_TX_2_evaluation',\n",
       " 996: 'FOODS_1_102_TX_3_evaluation',\n",
       " 997: 'FOODS_1_102_WI_1_evaluation',\n",
       " 998: 'FOODS_1_102_WI_2_evaluation',\n",
       " 999: 'FOODS_1_102_WI_3_evaluation',\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = pd.read_pickle('label_encoding.pickle')\n",
    "new_dict = dict([(value, key) for key, value in le['id'].items()])\n",
    "new_dict #mapping label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sale</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>...</th>\n",
       "      <th>roll_std_14</th>\n",
       "      <th>roll_mean_28</th>\n",
       "      <th>roll_std_28</th>\n",
       "      <th>roll_mean_49</th>\n",
       "      <th>roll_std_49</th>\n",
       "      <th>expanding_mean_item</th>\n",
       "      <th>mean_id_sold</th>\n",
       "      <th>std_id_sold</th>\n",
       "      <th>mean_id_price</th>\n",
       "      <th>std_id_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>1437</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1914</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841797</td>\n",
       "      <td>1.036133</td>\n",
       "      <td>0.922363</td>\n",
       "      <td>1.061523</td>\n",
       "      <td>1.143555</td>\n",
       "      <td>0.3044</td>\n",
       "      <td>0.321533</td>\n",
       "      <td>0.704102</td>\n",
       "      <td>4.515625</td>\n",
       "      <td>4.128906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>1438</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1914</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497314</td>\n",
       "      <td>0.357178</td>\n",
       "      <td>0.488037</td>\n",
       "      <td>0.265381</td>\n",
       "      <td>0.446045</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.565430</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>1.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>1439</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1914</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425781</td>\n",
       "      <td>0.535645</td>\n",
       "      <td>1.201172</td>\n",
       "      <td>0.469482</td>\n",
       "      <td>0.959473</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.156982</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.475586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1914</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>1.892578</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>1.877930</td>\n",
       "      <td>1.844727</td>\n",
       "      <td>1.7170</td>\n",
       "      <td>1.694336</td>\n",
       "      <td>1.983398</td>\n",
       "      <td>4.449219</td>\n",
       "      <td>0.615723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>1441</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1914</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099609</td>\n",
       "      <td>1.107422</td>\n",
       "      <td>0.994141</td>\n",
       "      <td>1.142578</td>\n",
       "      <td>1.136719</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>1.288086</td>\n",
       "      <td>2.773438</td>\n",
       "      <td>0.696777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  item_id  dept_id  cat_id  store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation     1437        3       1         0   \n",
       "1  HOBBIES_1_002_CA_1_validation     1438        3       1         0   \n",
       "2  HOBBIES_1_003_CA_1_validation     1439        3       1         0   \n",
       "3  HOBBIES_1_004_CA_1_validation     1440        3       1         0   \n",
       "4  HOBBIES_1_005_CA_1_validation     1441        3       1         0   \n",
       "\n",
       "   state_id     d  sale  year  event_name_1  ...  roll_std_14  roll_mean_28  \\\n",
       "0         0  1914     0     6             0  ...     0.841797      1.036133   \n",
       "1         0  1914     0     6             0  ...     0.497314      0.357178   \n",
       "2         0  1914     0     6             0  ...     0.425781      0.535645   \n",
       "3         0  1914     0     6             0  ...     2.173828      1.892578   \n",
       "4         0  1914     1     6             0  ...     1.099609      1.107422   \n",
       "\n",
       "   roll_std_28  roll_mean_49  roll_std_49  expanding_mean_item  mean_id_sold  \\\n",
       "0     0.922363      1.061523     1.143555               0.3044      0.321533   \n",
       "1     0.488037      0.265381     0.446045               0.2610      0.253906   \n",
       "2     1.201172      0.469482     0.959473               0.1442      0.156982   \n",
       "3     1.968750      1.877930     1.844727               1.7170      1.694336   \n",
       "4     0.994141      1.142578     1.136719               0.9610      0.958984   \n",
       "\n",
       "   std_id_sold  mean_id_price  std_id_price  \n",
       "0     0.704102       4.515625      4.128906  \n",
       "1     0.565430       3.687500      1.020508  \n",
       "2     0.498291       1.312500      1.475586  \n",
       "3     1.983398       4.449219      0.615723  \n",
       "4     1.288086       2.773438      0.696777  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_df.csv',dtype = d)\n",
    "test = test.replace({'id':new_dict})\n",
    "test['id'] = test['id'].map(lambda x: x.replace('evaluation','validation')) #Changed required for kaggle submission\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['d'] = test['d'].astype(str).map(lambda x: x.replace(x,'d_'+str(x)))\n",
    "test['sale'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sale</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>...</th>\n",
       "      <th>roll_std_14</th>\n",
       "      <th>roll_mean_28</th>\n",
       "      <th>roll_std_28</th>\n",
       "      <th>roll_mean_49</th>\n",
       "      <th>roll_std_49</th>\n",
       "      <th>expanding_mean_item</th>\n",
       "      <th>mean_id_sold</th>\n",
       "      <th>std_id_sold</th>\n",
       "      <th>mean_id_price</th>\n",
       "      <th>std_id_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>1437</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1914</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841797</td>\n",
       "      <td>1.036133</td>\n",
       "      <td>0.922363</td>\n",
       "      <td>1.061523</td>\n",
       "      <td>1.143555</td>\n",
       "      <td>0.3044</td>\n",
       "      <td>0.321533</td>\n",
       "      <td>0.704102</td>\n",
       "      <td>4.515625</td>\n",
       "      <td>4.128906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>1438</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1914</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497314</td>\n",
       "      <td>0.357178</td>\n",
       "      <td>0.488037</td>\n",
       "      <td>0.265381</td>\n",
       "      <td>0.446045</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.565430</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>1.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>1439</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1914</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425781</td>\n",
       "      <td>0.535645</td>\n",
       "      <td>1.201172</td>\n",
       "      <td>0.469482</td>\n",
       "      <td>0.959473</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.156982</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.475586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1914</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>1.892578</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>1.877930</td>\n",
       "      <td>1.844727</td>\n",
       "      <td>1.7170</td>\n",
       "      <td>1.694336</td>\n",
       "      <td>1.983398</td>\n",
       "      <td>4.449219</td>\n",
       "      <td>0.615723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>1441</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1914</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099609</td>\n",
       "      <td>1.107422</td>\n",
       "      <td>0.994141</td>\n",
       "      <td>1.142578</td>\n",
       "      <td>1.136719</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>1.288086</td>\n",
       "      <td>2.773438</td>\n",
       "      <td>0.696777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  item_id  dept_id  cat_id  store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation     1437        3       1         0   \n",
       "1  HOBBIES_1_002_CA_1_validation     1438        3       1         0   \n",
       "2  HOBBIES_1_003_CA_1_validation     1439        3       1         0   \n",
       "3  HOBBIES_1_004_CA_1_validation     1440        3       1         0   \n",
       "4  HOBBIES_1_005_CA_1_validation     1441        3       1         0   \n",
       "\n",
       "   state_id       d      sale  year  event_name_1  ...  roll_std_14  \\\n",
       "0         0  d_1914  1.236179     6             0  ...     0.841797   \n",
       "1         0  d_1914  1.236179     6             0  ...     0.497314   \n",
       "2         0  d_1914  1.236179     6             0  ...     0.425781   \n",
       "3         0  d_1914  1.236179     6             0  ...     2.173828   \n",
       "4         0  d_1914  1.236179     6             0  ...     1.099609   \n",
       "\n",
       "   roll_mean_28  roll_std_28  roll_mean_49  roll_std_49  expanding_mean_item  \\\n",
       "0      1.036133     0.922363      1.061523     1.143555               0.3044   \n",
       "1      0.357178     0.488037      0.265381     0.446045               0.2610   \n",
       "2      0.535645     1.201172      0.469482     0.959473               0.1442   \n",
       "3      1.892578     1.968750      1.877930     1.844727               1.7170   \n",
       "4      1.107422     0.994141      1.142578     1.136719               0.9610   \n",
       "\n",
       "   mean_id_sold  std_id_sold  mean_id_price  std_id_price  \n",
       "0      0.321533     0.704102       4.515625      4.128906  \n",
       "1      0.253906     0.565430       3.687500      1.020508  \n",
       "2      0.156982     0.498291       1.312500      1.475586  \n",
       "3      1.694336     1.983398       4.449219      0.615723  \n",
       "4      0.958984     1.288086       2.773438      0.696777  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sale</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>...</th>\n",
       "      <th>roll_std_14</th>\n",
       "      <th>roll_mean_28</th>\n",
       "      <th>roll_std_28</th>\n",
       "      <th>roll_mean_49</th>\n",
       "      <th>roll_std_49</th>\n",
       "      <th>expanding_mean_item</th>\n",
       "      <th>mean_id_sold</th>\n",
       "      <th>std_id_sold</th>\n",
       "      <th>mean_id_price</th>\n",
       "      <th>std_id_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>1437</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1942</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>0.928711</td>\n",
       "      <td>1.152344</td>\n",
       "      <td>1.020508</td>\n",
       "      <td>1.070312</td>\n",
       "      <td>0.3135</td>\n",
       "      <td>0.321533</td>\n",
       "      <td>0.704102</td>\n",
       "      <td>4.515625</td>\n",
       "      <td>4.128906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>1438</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1942</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267334</td>\n",
       "      <td>0.035706</td>\n",
       "      <td>0.188965</td>\n",
       "      <td>0.163208</td>\n",
       "      <td>0.373535</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.565430</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>1.020508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>1439</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1942</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679199</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0.689941</td>\n",
       "      <td>0.469482</td>\n",
       "      <td>0.648926</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.156982</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.475586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>1440</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1942</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.199219</td>\n",
       "      <td>1.821289</td>\n",
       "      <td>1.886719</td>\n",
       "      <td>1.917969</td>\n",
       "      <td>1.923828</td>\n",
       "      <td>1.7180</td>\n",
       "      <td>1.694336</td>\n",
       "      <td>1.983398</td>\n",
       "      <td>4.449219</td>\n",
       "      <td>0.615723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>1441</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1942</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>1.357422</td>\n",
       "      <td>1.283203</td>\n",
       "      <td>1.286133</td>\n",
       "      <td>1.172852</td>\n",
       "      <td>0.9670</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>1.288086</td>\n",
       "      <td>2.773438</td>\n",
       "      <td>0.696777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  item_id  dept_id  cat_id  store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation     1437        3       1         0   \n",
       "1  HOBBIES_1_002_CA_1_evaluation     1438        3       1         0   \n",
       "2  HOBBIES_1_003_CA_1_evaluation     1439        3       1         0   \n",
       "3  HOBBIES_1_004_CA_1_evaluation     1440        3       1         0   \n",
       "4  HOBBIES_1_005_CA_1_evaluation     1441        3       1         0   \n",
       "\n",
       "   state_id       d      sale  year  event_name_1  ...  roll_std_14  \\\n",
       "0         0  d_1942  1.236179     6             0  ...     0.997070   \n",
       "1         0  d_1942  1.236179     6             0  ...     0.267334   \n",
       "2         0  d_1942  1.236179     6             0  ...     0.679199   \n",
       "3         0  d_1942  1.236179     6             0  ...     2.199219   \n",
       "4         0  d_1942  1.236179     6             0  ...     1.007812   \n",
       "\n",
       "   roll_mean_28  roll_std_28  roll_mean_49  roll_std_49  expanding_mean_item  \\\n",
       "0      0.928711     1.152344      1.020508     1.070312               0.3135   \n",
       "1      0.035706     0.188965      0.163208     0.373535               0.2576   \n",
       "2      0.571289     0.689941      0.469482     0.648926               0.1505   \n",
       "3      1.821289     1.886719      1.917969     1.923828               1.7180   \n",
       "4      1.357422     1.283203      1.286133     1.172852               0.9670   \n",
       "\n",
       "   mean_id_sold  std_id_sold  mean_id_price  std_id_price  \n",
       "0      0.321533     0.704102       4.515625      4.128906  \n",
       "1      0.253906     0.565430       3.687500      1.020508  \n",
       "2      0.156982     0.498291       1.312500      1.475586  \n",
       "3      1.694336     1.983398       4.449219      0.615723  \n",
       "4      0.958984     1.288086       2.773438      0.696777  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f = pd.read_csv('final_sub.csv',dtype = d)\n",
    "test_f = test_f.replace({'id':new_dict})\n",
    "test_f['d'] = test_f['d'].astype(str).map(lambda x: x.replace(x,'d_'+str(x)))\n",
    "test_f['sale'] = y_pred_f\n",
    "test_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30490it [59:59,  8.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "F = ['F' + str(i) for i in range(1,29)]\n",
    "for t1,t2 in tqdm(zip(test['id'].unique(),test_f['id'].unique())): #uploading all sales value to submission file\n",
    "    df.loc[df['id']==t1,F] = test[test['id']==t1].sort_values('d')['sale'].tolist()\n",
    "    df.loc[df['id']==t2,F] = test_f[test_f['id']==t2].sort_values('d')['sale'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>...</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>...</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>...</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>...</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>...</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "      <td>1.236179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        F1        F2        F3        F4  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  1.236179  1.236179  1.236179  1.236179   \n",
       "1  HOBBIES_1_002_CA_1_validation  1.236179  1.236179  1.236179  1.236179   \n",
       "2  HOBBIES_1_003_CA_1_validation  1.236179  1.236179  1.236179  1.236179   \n",
       "3  HOBBIES_1_004_CA_1_validation  1.236179  1.236179  1.236179  1.236179   \n",
       "4  HOBBIES_1_005_CA_1_validation  1.236179  1.236179  1.236179  1.236179   \n",
       "\n",
       "         F5        F6        F7        F8        F9  ...       F19       F20  \\\n",
       "0  1.236179  1.236179  1.236179  1.236179  1.236179  ...  1.236179  1.236179   \n",
       "1  1.236179  1.236179  1.236179  1.236179  1.236179  ...  1.236179  1.236179   \n",
       "2  1.236179  1.236179  1.236179  1.236179  1.236179  ...  1.236179  1.236179   \n",
       "3  1.236179  1.236179  1.236179  1.236179  1.236179  ...  1.236179  1.236179   \n",
       "4  1.236179  1.236179  1.236179  1.236179  1.236179  ...  1.236179  1.236179   \n",
       "\n",
       "        F21       F22       F23       F24       F25       F26       F27  \\\n",
       "0  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179   \n",
       "1  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179   \n",
       "2  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179   \n",
       "3  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179   \n",
       "4  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179  1.236179   \n",
       "\n",
       "        F28  \n",
       "0  1.236179  \n",
       "1  1.236179  \n",
       "2  1.236179  \n",
       "3  1.236179  \n",
       "4  1.236179  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_submission_RF.csv',index=False) #saving uploaded predicted sale value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For Default Objective function we get negative prediction. So clipping those to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\"Counting number of negative values\"\n",
    "df = pd.read_csv('final_submission_xgbr.csv')\n",
    "count = 0\n",
    "for ele in df.columns:\n",
    "    if ele !='id':\n",
    "        for ele2 in df[ele]:\n",
    "            #print(ele2,ele)\n",
    "            if ele2<0:\n",
    "                count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\"Negative values clipping to zero\"\n",
    "df = pd.read_csv('final_submission_LGBM_cat.csv')\n",
    "for ele in df.columns:\n",
    "    if ele !='id':\n",
    "        for ele2,ids in zip(df[ele],df['id']):\n",
    "            #print(ele2,ele)\n",
    "            if ele2<0:\n",
    "                df.loc[df['id']==ids,ele] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\"checking numnber of negative value after clipping\"\n",
    "#df = pd.read_csv('final_submission_LGBM.csv')\n",
    "count = 0\n",
    "for ele in df.columns:\n",
    "    if ele !='id':\n",
    "        for ele2 in df[ele]:\n",
    "            #print(ele2,ele)\n",
    "            if ele2<0:\n",
    "                count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('LGBM_clipping_neg2zero.csv',index=False) #saving clipped values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+---------------+\n",
      "|      Model       | Test/CV RMSE Value | Private Score |\n",
      "+------------------+--------------------+---------------+\n",
      "|   Linear Model   |        2.24        |    1.35166    |\n",
      "|     DT Model     |        3.44        |       -       |\n",
      "|   Linear Model   |        3.64        |    2.27301    |\n",
      "|     XGBoost      |        2.17        |    0.67483    |\n",
      "| LightGBM_Tweedie |        2.16        |    0.68122    |\n",
      "|     LightGBM     |        2.14        |    0.64101    |\n",
      "|  Dept_LightGBM   |        2.15        |    0.69979    |\n",
      "+------------------+--------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "  \n",
    "# Specify the Column Names while initializing the Table\n",
    "myTable = PrettyTable([\"Model\", \"Test/CV RMSE Value\",\"Private Score\"])\n",
    "  \n",
    "# Add rows\n",
    "myTable.add_row([\"Linear Model\", \"2.24\",\"1.35166\"])\n",
    "myTable.add_row([\"DT Model\", \"3.44\",\"-\"])\n",
    "myTable.add_row([\"Linear Model\", \"3.64\",\"2.27301\"])\n",
    "myTable.add_row([\"XGBoost\", \"2.17\",\"0.67483\"])\n",
    "myTable.add_row([\"LightGBM_Tweedie\", \"2.16\",\"0.68122\"])\n",
    "myTable.add_row([\"LightGBM\", \"2.14\",\"0.64101\"])\n",
    "myTable.add_row([\"Dept_LightGBM\", \"2.15\",\"0.69979\"])\n",
    "\n",
    "print(myTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of HACKANONS COLAB 25GB RAM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
